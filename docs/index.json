[{"author":null,"categories":["Tech"],"content":"机器学习作为人工智能的一个重要分支，在近年来取得了显著的发展。神经网络，作为机器学习中的核心技术之一，已广泛应用于图像识别、语音处理、自然语言处理等领域。本文旨在深入浅出地解析神经网络的基础原理，以及其在各行各业中的应用实例、代码实战","date":1704931200,"description":"机器学习作为人工智能的一个重要分支，在近年来取得了显著的发展。神经网络，作为机器学习中的核心技术之一，已广泛应用于图像识别、语音处理、自然语言处理等领域。本文旨在深入浅出地解析神经网络的基础原理，以及其在各行各业中的应用实例、代码实战","dir":"post/","excerpt_html":"机器学习作为人工智能的一个重要分支，在近年来取得了显著的发展。神经网络，作为机器学习中的核心技术之一，已广泛应用于图像识别、语音处理、自然语言处理等领域。本文旨在深入浅出地解析神经网络的基础原理，以及其在各行各业中的应用实例、代码实战","excerpt_text":"机器学习作为人工智能的一个重要分支，在近年来取得了显著的发展。神经网络，作为机器学习中的核心技术之一，已广泛应用于图像识别、语音处理、自然语言处理等领域。本文旨在深入浅出地解析神经网络的基础原理，以及其在各行各业中的应用实例、代码实战","expirydate":-62135596800,"fuzzywordcount":2600,"html":"机器学习作为人工智能的一个重要分支，在近年来取得了显著的发展。神经网络，作为机器学习中的核心技术之一，已广泛应用于图像识别、语音处理、自然语言处理等领域。本文旨在深入浅出地解析神经网络的基础原理，以及其在各行各业中的应用实例、代码实战","keywords":null,"kind":"page","lang":"en","lastmod":1704931200,"objectID":"063c864a381aea5a4f36f3e010fc5baa","permalink":"https://www.devean.cn/post/2024-01-11-machine-learning-neural-networks/","publishdate":"2024-01-11T00:00:00Z","readingtime":6,"relpermalink":"/post/2024-01-11-machine-learning-neural-networks/","section":"post","summary":"机器学习作为人工智能的一个重要分支，在近年来取得了显著的发展。神经网络，作为机器学习中的核心技术之一，已广泛应用于图像识别、语音处理、自然语","tags":["Machine Learning"],"title":"神经网络 | 机器学习","type":"post","url":"/post/2024-01-11-machine-learning-neural-networks/","weight":0,"wordcount":2535},{"author":null,"categories":["Tech"],"content":"机器学习领域中，决策树是一种强大的算法，被广泛应用于分类和回归问题。本文将深入探讨决策树的概念、原理、流程、工业应用场景，并通过代码实践展示其实现。","date":1704067200,"description":"机器学习领域中，决策树是一种强大的算法，被广泛应用于分类和回归问题。本文将深入探讨决策树的概念、原理、流程、工业应用场景，并通过代码实践展示其实现。","dir":"post/","excerpt_html":"机器学习领域中，决策树是一种强大的算法，被广泛应用于分类和回归问题。本文将深入探讨决策树的概念、原理、流程、工业应用场景，并通过代码实践展示其实现。","excerpt_text":"机器学习领域中，决策树是一种强大的算法，被广泛应用于分类和回归问题。本文将深入探讨决策树的概念、原理、流程、工业应用场景，并通过代码实践展示其实现。","expirydate":-62135596800,"fuzzywordcount":2000,"html":"机器学习领域中，决策树是一种强大的算法，被广泛应用于分类和回归问题。本文将深入探讨决策树的概念、原理、流程、工业应用场景，并通过代码实践展示其实现。","keywords":null,"kind":"page","lang":"en","lastmod":1704067200,"objectID":"fd8740d82e469d2406470defafa49951","permalink":"https://www.devean.cn/post/2024-01-01-machine-learning-decision-trees/","publishdate":"2024-01-01T00:00:00Z","readingtime":4,"relpermalink":"/post/2024-01-01-machine-learning-decision-trees/","section":"post","summary":"引言 机器学习领域中，决策树是一种强大的算法，被广泛应用于分类和回归问题。本文将深入探讨决策树的概念、原理、流程、工业应用场景，并通过代码实践","tags":["Machine Learning"],"title":"决策树 | 机器学习","type":"post","url":"/post/2024-01-01-machine-learning-decision-trees/","weight":0,"wordcount":1943},{"author":null,"categories":["Tech"],"content":"在机器学习的广阔领域中，自组织映射（Self-Organizing Map，SOM）占据了一席之地。它是一种无监督学习的人工神经网络，用于数据的降维和可视化。今天，我们将深入探讨SOM的概念、定理、原理，并通过Python示例展示其在实际问题中的应用","date":1702944000,"description":"在机器学习的广阔领域中，自组织映射（Self-Organizing Map，SOM）占据了一席之地。它是一种无监督学习的人工神经网络，用于数据的降维和可视化。今天，我们将深入探讨SOM的概念、定理、原理，并通过Python示例展示其在实际问题中的应用","dir":"post/","excerpt_html":"在机器学习的广阔领域中，自组织映射（Self-Organizing Map，SOM）占据了一席之地。它是一种无监督学习的人工神经网络，用于数据的降维和可视化。今天，我们将深入探讨SOM的概念、定理、原理，并通过Python示例展示其在实际问题中的应用","excerpt_text":"在机器学习的广阔领域中，自组织映射（Self-Organizing Map，SOM）占据了一席之地。它是一种无监督学习的人工神经网络，用于数据的降维和可视化。今天，我们将深入探讨SOM的概念、定理、原理，并通过Python示例展示其在实际问题中的应用","expirydate":-62135596800,"fuzzywordcount":1700,"html":"在机器学习的广阔领域中，自组织映射（Self-Organizing Map，SOM）占据了一席之地。它是一种无监督学习的人工神经网络，用于数据的降维和可视化。今天，我们将深入探讨SOM的概念、定理、原理，并通过Python示例展示其在实际问题中的应用","keywords":null,"kind":"page","lang":"en","lastmod":1702944000,"objectID":"8dfb87f742fda4082d302aa6e2717b6b","permalink":"https://www.devean.cn/post/2023-12-19-machine-learning-self-organnizing-maps/","publishdate":"2023-12-19T00:00:00Z","readingtime":4,"relpermalink":"/post/2023-12-19-machine-learning-self-organnizing-maps/","section":"post","summary":"引言 在机器学习的广阔领域中，自组织映射（Self-Organizing Map，SOM）占据了一席之地。它是一种无监督学习的人工神经网络，用于","tags":["Machine Learning"],"title":"自组织映射 | 机器学习","type":"post","url":"/post/2023-12-19-machine-learning-self-organnizing-maps/","weight":0,"wordcount":1652},{"author":null,"categories":["Tech"],"content":"非线性支持向量机的核技巧、核函数、决策函数，股价预测模型、总结支持向量机目标函数","date":1701475200,"description":"非线性支持向量机的核技巧、核函数、决策函数，股价预测模型、总结支持向量机目标函数","dir":"post/","excerpt_html":"非线性支持向量机的核技巧、核函数、决策函数，股价预测模型、总结支持向量机目标函数","excerpt_text":"非线性支持向量机的核技巧、核函数、决策函数，股价预测模型、总结支持向量机目标函数","expirydate":-62135596800,"fuzzywordcount":2400,"html":"非线性支持向量机的核技巧、核函数、决策函数，股价预测模型、总结支持向量机目标函数","keywords":null,"kind":"page","lang":"en","lastmod":1701475200,"objectID":"77cf87d983ce0ac86a3bf4a52ff23a5a","permalink":"https://www.devean.cn/post/2023-12-02-machine-learning-nonlinear-support-vector-machines/","publishdate":"2023-12-02T00:00:00Z","readingtime":5,"relpermalink":"/post/2023-12-02-machine-learning-nonlinear-support-vector-machines/","section":"post","summary":"非线性支持向量机（SVM）是一种强大的监督学习算法，用于解决分类和回归问题。它通过使用核技巧将数据映射到高维空间，从而处理非线性关系。在这篇","tags":["Machine Learning"],"title":"机器学习 | 非线性支持向量机","type":"post","url":"/post/2023-12-02-machine-learning-nonlinear-support-vector-machines/","weight":0,"wordcount":2311},{"author":null,"categories":["Tech"],"content":"支持向量机线性不可分、软间隔、松弛变量、目标函数、约束条件、超参数 C,实际应用场景判别线性是否可分等几方面讲概述了支持向量机线性不可分","date":1700352000,"description":"支持向量机线性不可分、软间隔、松弛变量、目标函数、约束条件、超参数 C,实际应用场景判别线性是否可分等几方面讲概述了支持向量机线性不可分","dir":"post/","excerpt_html":"支持向量机线性不可分、软间隔、松弛变量、目标函数、约束条件、超参数 C,实际应用场景判别线性是否可分等几方面讲概述了支持向量机线性不可分","excerpt_text":"支持向量机线性不可分、软间隔、松弛变量、目标函数、约束条件、超参数 C,实际应用场景判别线性是否可分等几方面讲概述了支持向量机线性不可分","expirydate":-62135596800,"fuzzywordcount":1900,"html":"支持向量机线性不可分、软间隔、松弛变量、目标函数、约束条件、超参数 C,实际应用场景判别线性是否可分等几方面讲概述了支持向量机线性不可分","keywords":null,"kind":"page","lang":"en","lastmod":1700352000,"objectID":"c49cd9a9e6adbb540a0116a0105ef7a5","permalink":"https://www.devean.cn/post/2023-11-19-machine-learning-support-vector-machine-non-linearly-separable/","publishdate":"2023-11-19T00:00:00Z","readingtime":4,"relpermalink":"/post/2023-11-19-machine-learning-support-vector-machine-non-linearly-separable/","section":"post","summary":"本文从支持向量机线性不可分、软间隔、松弛变量、目标函数、约束条件、超参数 C,实际应用场景判别线性是否可分等几方面讲概述了支持向量机线性不可分","tags":["Machine Learning"],"title":"机器学习 | 支持向量机线性不可分","type":"post","url":"/post/2023-11-19-machine-learning-support-vector-machine-non-linearly-separable/","weight":0,"wordcount":1819},{"author":null,"categories":["Tech"],"content":"支持向量机概念、硬间隔、软间隔和非线性的区别、原理、术语、最大间隔数学推导几个方面详细讲解线性可分的支持向量机","date":1700265600,"description":"支持向量机概念、硬间隔、软间隔和非线性的区别、原理、术语、最大间隔数学推导几个方面详细讲解线性可分的支持向量机","dir":"post/","excerpt_html":"支持向量机概念、硬间隔、软间隔和非线性的区别、原理、术语、最大间隔数学推导几个方面详细讲解线性可分的支持向量机","excerpt_text":"支持向量机概念、硬间隔、软间隔和非线性的区别、原理、术语、最大间隔数学推导几个方面详细讲解线性可分的支持向量机","expirydate":-62135596800,"fuzzywordcount":1900,"html":"支持向量机概念、硬间隔、软间隔和非线性的区别、原理、术语、最大间隔数学推导几个方面详细讲解线性可分的支持向量机","keywords":null,"kind":"page","lang":"en","lastmod":1700265600,"objectID":"fc110a56be710a12dfd5c35c78fb3545","permalink":"https://www.devean.cn/post/2023-11-18-machine-learning-support-vector-machine-linearly-separable/","publishdate":"2023-11-18T00:00:00Z","readingtime":4,"relpermalink":"/post/2023-11-18-machine-learning-support-vector-machine-linearly-separable/","section":"post","summary":"本文从支持向量机概念、硬间隔、软间隔和非线性的区别、原理、术语、最大间隔数学推导几个方面详细讲解线性可分的支持向量机。 基础概念 支持向量机(S","tags":["Machine Learning"],"title":"机器学习 | 支持向量机线性可分","type":"post","url":"/post/2023-11-18-machine-learning-support-vector-machine-linearly-separable/","weight":0,"wordcount":1802},{"author":null,"categories":["Finance"],"content":"简单科普了什么是套利、有哪些套利方法、套利失效的原因。","date":1700060371,"description":"简单科普了什么是套利、有哪些套利方法、套利失效的原因。","dir":"post/","excerpt_html":"简单科普了什么是套利、有哪些套利方法、套利失效的原因。","excerpt_text":"简单科普了什么是套利、有哪些套利方法、套利失效的原因。","expirydate":-62135596800,"fuzzywordcount":900,"html":"简单科普了什么是套利、有哪些套利方法、套利失效的原因。","keywords":["套利","Arbitrage","量化金融","Quantitative Finance"],"kind":"page","lang":"en","lastmod":1700060371,"objectID":"702a6aab9686b421c6068d90f9aa620a","permalink":"https://www.devean.cn/post/2023-11-15-what-is-arbitrage/","publishdate":"2023-11-15T22:59:31+08:00","readingtime":2,"relpermalink":"/post/2023-11-15-what-is-arbitrage/","section":"post","summary":"本文简单科普了什么是套利、有哪些套利方法、套利失效的原因。 什么是套利 套利是指在无风险回报率之外获得确定的利润。 用量化金融的语言来说，我们 可以","tags":["Quantitative Finance"],"title":"什么是套利 | 翻译","type":"post","url":"/post/2023-11-15-what-is-arbitrage/","weight":0,"wordcount":835},{"author":null,"categories":["Finance"],"content":"量化金融中数学的应用、常见的建模方法、常见的数学工具普及了量化金融中的应用","date":1699973971,"description":"量化金融中数学的应用、常见的建模方法、常见的数学工具普及了量化金融中的应用","dir":"post/","excerpt_html":"量化金融中数学的应用、常见的建模方法、常见的数学工具普及了量化金融中的应用","excerpt_text":"量化金融中数学的应用、常见的建模方法、常见的数学工具普及了量化金融中的应用","expirydate":-62135596800,"fuzzywordcount":400,"html":"量化金融中数学的应用、常见的建模方法、常见的数学工具普及了量化金融中的应用","keywords":null,"kind":"page","lang":"en","lastmod":1699973971,"objectID":"6a3aebed5e26a09f8b3f17ba5f95c8b8","permalink":"https://www.devean.cn/post/2023-11-14-mathematical-applications-in-quantitative-finance/","publishdate":"2023-11-14T22:59:31+08:00","readingtime":1,"relpermalink":"/post/2023-11-14-mathematical-applications-in-quantitative-finance/","section":"post","summary":"本文量从量化金融中数学的应用、常见的建模方法、常见的数学工具普及了量化金融中的应用。 量化金融中有哪些不同类型的数学？ 在数量金融中使用最多的数","tags":["Quantitative Finance"],"title":"量化金融中的数学应用 | 翻译","type":"post","url":"/post/2023-11-14-mathematical-applications-in-quantitative-finance/","weight":0,"wordcount":379},{"author":null,"categories":["Tech"],"content":"特征缩放(Feature Scaling)概念、目的、常用特征缩放方法：最小-最大缩放、标准缩放、鲁棒缩放、L2 Normalization、L1 Normalization、Power Transformer的公式讲解、Python缩放数据可视化对比诠释了特征缩放","date":1699660800,"description":"特征缩放(Feature Scaling)概念、目的、常用特征缩放方法：最小-最大缩放、标准缩放、鲁棒缩放、L2 Normalization、L1 Normalization、Power Transformer的公式讲解、Python缩放数据可视化对比诠释了特征缩放","dir":"post/","excerpt_html":"特征缩放(Feature Scaling)概念、目的、常用特征缩放方法：最小-最大缩放、标准缩放、鲁棒缩放、L2 Normalization、L1 Normalization、Power Transformer的公式讲解、Python缩放数据可视化对比诠释了特征缩放","excerpt_text":"特征缩放(Feature Scaling)概念、目的、常用特征缩放方法：最小-最大缩放、标准缩放、鲁棒缩放、L2 Normalization、L1 Normalization、Power Transformer的公式讲解、Python缩放数据可视化对比诠释了特征缩放","expirydate":-62135596800,"fuzzywordcount":1900,"html":"特征缩放(Feature Scaling)概念、目的、常用特征缩放方法：最小-最大缩放、标准缩放、鲁棒缩放、L2 Normalization、L1 Normalization、Power Transformer的公式讲解、Python缩放数据可视化对比诠释了特征缩放","keywords":null,"kind":"page","lang":"en","lastmod":1699660800,"objectID":"a3d1bc0297d485650c412b0fe2cfd17a","permalink":"https://www.devean.cn/post/2023-11-11-machine-learning-feature-scaling/","publishdate":"2023-11-11T00:00:00Z","readingtime":4,"relpermalink":"/post/2023-11-11-machine-learning-feature-scaling/","section":"post","summary":"\u003cblockquote\u003e\n\u003cp\u003e本文从特征缩放概念、目的、常用特征缩放方法：最小-最大缩放、标准缩放、鲁棒缩放、L2 Normalization、L1 Normalization、Power Transformer的公式讲解、Python缩放数据可视化对比诠释了特征缩放\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2 id=\"什么是特征缩放\"\u003e什么是特征缩放\u003c/h2\u003e\n\u003cp\u003e特征缩放又称归一化，是机器学习中的一种技术，涉及调整数值数据的量度，使所有数据点在相似的尺度上。例如：身高、体重、年龄、收入等个人特征数据，每个维度的区间不一样，为保证所有维度的特征数据尺度一样，我们就需要对原始数据做特征缩放，将身高、体重、年龄、收入都转化为区间[0,1]之间的数据。\u003c/p\u003e","tags":["Machine Learning"],"title":"机器学习 | 特征缩放","type":"post","url":"/post/2023-11-11-machine-learning-feature-scaling/","weight":0,"wordcount":1829},{"author":null,"categories":["Tech"],"content":"K临近(KNN)的概念、原理、距离函数、K 值选择、K 值影响、、优缺点、应用几方面详细讲述了 KNN 算法","date":1694304000,"description":"K临近(KNN)的概念、原理、距离函数、K 值选择、K 值影响、、优缺点、应用几方面详细讲述了 KNN 算法","dir":"post/","excerpt_html":"K临近(KNN)的概念、原理、距离函数、K 值选择、K 值影响、、优缺点、应用几方面详细讲述了 KNN 算法","excerpt_text":"K临近(KNN)的概念、原理、距离函数、K 值选择、K 值影响、、优缺点、应用几方面详细讲述了 KNN 算法","expirydate":-62135596800,"fuzzywordcount":2100,"html":"K临近(KNN)的概念、原理、距离函数、K 值选择、K 值影响、、优缺点、应用几方面详细讲述了 KNN 算法","keywords":null,"kind":"page","lang":"en","lastmod":1694304000,"objectID":"80b02e6cd32e266756e5dd3450c43a77","permalink":"https://www.devean.cn/post/2023-11-12-machine-learning-k-nearest-neighbours/","publishdate":"2023-09-10T00:00:00Z","readingtime":5,"relpermalink":"/post/2023-11-12-machine-learning-k-nearest-neighbours/","section":"post","summary":"本文从概念、原理、距离函数、K 值选择、K 值影响、、优缺点、应用几方面详细讲述了 KNN 算法。 K 近临(K Nearest-Neighbours) 一种简单的监督学习算法，惰性学习算法，在技","tags":["Machine Learning"],"title":"K临近(KNN) | 机器学习","type":"post","url":"/post/2023-11-12-machine-learning-k-nearest-neighbours/","weight":0,"wordcount":2091},{"author":null,"categories":["Tech"],"content":"机器学习与传统建模区别、机器学习分类：监督、非监督、半监督、强化，基础算法：K临近、K均值、朴素贝叶斯、支持向量机、回归、自组织映射、神经网络原理","date":1694131200,"description":"机器学习与传统建模区别、机器学习分类：监督、非监督、半监督、强化，基础算法：K临近、K均值、朴素贝叶斯、支持向量机、回归、自组织映射、神经网络原理","dir":"post/","excerpt_html":"机器学习与传统建模区别、机器学习分类：监督、非监督、半监督、强化，基础算法：K临近、K均值、朴素贝叶斯、支持向量机、回归、自组织映射、神经网络原理","excerpt_text":"机器学习与传统建模区别、机器学习分类：监督、非监督、半监督、强化，基础算法：K临近、K均值、朴素贝叶斯、支持向量机、回归、自组织映射、神经网络原理","expirydate":-62135596800,"fuzzywordcount":3800,"html":"机器学习与传统建模区别、机器学习分类：监督、非监督、半监督、强化，基础算法：K临近、K均值、朴素贝叶斯、支持向量机、回归、自组织映射、神经网络原理","keywords":null,"kind":"page","lang":"en","lastmod":1694131200,"objectID":"ee8bf0d96d30c4b78f4c18fe77e77bb8","permalink":"https://www.devean.cn/post/2023-09-08-machine-learning-basic-concepts/","publishdate":"2023-09-08T00:00:00Z","readingtime":8,"relpermalink":"/post/2023-09-08-machine-learning-basic-concepts/","section":"post","summary":"本文从机器学习与传统建模区别、机器学习分类：监督、非监督、半监督、强化，基础算法：K临近(KNN)、K均值(KMC)、朴素贝叶斯(NBC)、","tags":["Machine Learning"],"title":"机器学习 | 基础概念","type":"post","url":"/post/2023-09-08-machine-learning-basic-concepts/","weight":0,"wordcount":3770},{"author":null,"categories":["Life"],"content":"","date":1690934400,"description":"","dir":"post/","excerpt_html":"","excerpt_text":"","expirydate":-62135596800,"fuzzywordcount":400,"html":"","keywords":null,"kind":"page","lang":"en","lastmod":1690934400,"objectID":"32690ed761b13b3d7023afc15b8ca4d4","permalink":"https://www.devean.cn/post/2023-08-02-motorcycle-tour-around-xinjiang/","publishdate":"2023-08-02T00:00:00Z","readingtime":1,"relpermalink":"/post/2023-08-02-motorcycle-tour-around-xinjiang/","section":"post","summary":"环新疆摩旅 版权声明：以下图片版权归本人所有，禁止一切商用，如需商用请与本人取得联系获取授权，侵权必究！ 2023.8.2 北京\u0026ndash;巴彦淖尔临河服务区","tags":["Travel"],"title":"环新疆摩旅","type":"post","url":"/post/2023-08-02-motorcycle-tour-around-xinjiang/","weight":0,"wordcount":318},{"author":null,"categories":["Tech"],"content":"Nginx下载安装，完整配置、Web部署、免费CA证书配置全流程","date":1528070400,"description":"Nginx下载安装，完整配置、Web部署、免费CA证书配置全流程","dir":"post/","excerpt_html":"Nginx下载安装，完整配置、Web部署、免费CA证书配置全流程","excerpt_text":"Nginx下载安装，完整配置、Web部署、免费CA证书配置全流程","expirydate":-62135596800,"fuzzywordcount":300,"html":"Nginx下载安装，完整配置、Web部署、免费CA证书配置全流程","keywords":null,"kind":"page","lang":"en","lastmod":1528070400,"objectID":"a2da16a19a485b03f87c3c56bcdbe478","permalink":"https://www.devean.cn/post/2023-12-11-nginx%E9%83%A8%E7%BD%B2web%E6%9C%8D%E5%8A%A1%E5%8F%8A%E5%9F%9F%E5%90%8D%E5%85%8D%E8%B4%B9ca%E8%AE%A4%E8%AF%81%E8%AF%A6%E7%BB%86%E6%B5%81%E7%A8%8B/","publishdate":"2018-06-04T00:00:00Z","readingtime":1,"relpermalink":"/post/2023-12-11-nginx%E9%83%A8%E7%BD%B2web%E6%9C%8D%E5%8A%A1%E5%8F%8A%E5%9F%9F%E5%90%8D%E5%85%8D%E8%B4%B9ca%E8%AE%A4%E8%AF%81%E8%AF%A6%E7%BB%86%E6%B5%81%E7%A8%8B/","section":"post","summary":"Nginx 下载安装 Web服务部署 Nginx 详细配置 年费CA证书配置 安装certbot yum install certbot 手动只安装证书 certbot run -a manual -i nginx -d domain.com,www.domain.com Saving debug log to /var/log/letsencrypt/letsencrypt.log Requesting a certificate for www.domain.com - - - - - - - -","tags":["tag1","tag2"],"title":"Nginx部署web服务及域名免费CA认证详细流程","type":"post","url":"/post/2023-12-11-nginx%E9%83%A8%E7%BD%B2web%E6%9C%8D%E5%8A%A1%E5%8F%8A%E5%9F%9F%E5%90%8D%E5%85%8D%E8%B4%B9ca%E8%AE%A4%E8%AF%81%E8%AF%A6%E7%BB%86%E6%B5%81%E7%A8%8B/","weight":0,"wordcount":217},{"author":null,"categories":null,"content":null,"date":-62135596800,"description":"","dir":"about/","excerpt_html":null,"excerpt_text":null,"expirydate":-62135596800,"fuzzywordcount":100,"html":null,"keywords":null,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"8576ec274c98b3831668a172fa632d80","permalink":"https://www.devean.cn/about/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/about/","section":"","summary":"About Me Devean Ye is","tags":null,"title":"","type":"page","url":"/about/","weight":0,"wordcount":5},{"author":null,"categories":null,"content":null,"date":-62135596800,"description":"","dir":"notes/","excerpt_html":null,"excerpt_text":null,"expirydate":-62135596800,"fuzzywordcount":100,"html":null,"keywords":null,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"1ede8046f9c3a02d422dea7bbf324e64","permalink":"https://www.devean.cn/notes/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/notes/","section":"","summary":"Java 语言学习笔记","tags":null,"title":"","type":"page","url":"/notes/","weight":0,"wordcount":7},{"author":null,"categories":null,"content":null,"date":-62135596800,"description":"","dir":"search/","excerpt_html":null,"excerpt_text":null,"expirydate":-62135596800,"fuzzywordcount":100,"html":null,"keywords":null,"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"e70fff4f74e107cdcde5c9e2abcfc149","permalink":"https://www.devean.cn/search/","publishdate":"0001-01-01T00:00:00Z","readingtime":0,"relpermalink":"/search/","section":"search","summary":"","tags":null,"title":"","type":"search","url":"/search/","weight":0,"wordcount":0},{"author":null,"categories":null,"content":"Archive of historical posts.","date":-62135596800,"description":"Archive of historical posts.","dir":"archive/","excerpt_html":"Archive of historical posts.","excerpt_text":"Archive of historical posts.","expirydate":-62135596800,"fuzzywordcount":100,"html":"Archive of historical posts.","keywords":null,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"a06e5ce9eca4c3260843078104889780","permalink":"https://www.devean.cn/archive/","publishdate":"0001-01-01T00:00:00Z","readingtime":0,"relpermalink":"/archive/","section":"","summary":"","tags":null,"title":"Posts Archive","type":"archive","url":"/archive/","weight":0,"wordcount":0},{"author":null,"categories":null,"content":null,"date":-62135596800,"description":"","dir":"search/","excerpt_html":null,"excerpt_text":null,"expirydate":-62135596800,"fuzzywordcount":100,"html":null,"keywords":null,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"8946788897930c0c0c39fbfcd30ff2e4","permalink":"https://www.devean.cn/search/search/","publishdate":"0001-01-01T00:00:00Z","readingtime":0,"relpermalink":"/search/search/","section":"search","summary":"","tags":null,"title":"搜索","type":"search","url":"/search/search/","weight":0,"wordcount":0},{"contents":" 机器学习作为人工智能的一个重要分支，在近年来取得了显著的发展。神经网络，作为机器学习中的核心技术之一，已广泛应用于图像识别、语音处理、自然语言处理等领域。本文旨在深入浅出地解析神经网络的基础原理，以及其在各行各业中的应用实例、代码实战。\n神经网络基础 什么是神经网络 神经网络是一种模仿人类大脑结构设计的算法结构，它由大量的节点（或称为“神经元”）相互连接构成。每个神经元可以接收输入，进行处理后输出到其他神经元。这种结构使得神经网络能够处理复杂的数据模式。\n神经网络的工作原理 神经网络的工作可以分为两个主要阶段：前向传播和反向传播。在前向传播阶段，数据从输入层经过隐藏层到输出层，每一层的神经元对数据进行加权求和，再通过激活函数处理。在反向传播阶段，网络通过计算输出误差并将误差反向传播，以此来调整神经元的权重和偏置，不断优化网络性能。\n神经网络的类型 神经网络有多种类型，每种类型都适用于解决特定的问题。以下是一些主要类型的神经网络及其应用：\n卷积神经网络（CNN） 定义和用途：卷积神经网络主要应用于图像处理领域。它们通过模拟生物视觉系统的机制，有效地识别和处理图像数据。 结构特点：CNN主要由卷积层、池化层和全连接层组成。卷积层用于提取图像中的特征，池化层用于降低特征的维度，全连接层则用于分类或回归分析。 循环神经网络（RNN） 定义和用途：循环神经网络特别适用于处理序列数据，如时间序列分析、语音识别等。它们能够考虑到数据之间的时序关系。 结构特点：RNN的核心是它具有记忆功能，可以保存前一时刻的信息，并在当前时刻与新输入共同影响输出。 长短期记忆网络（LSTM） 定义和用途：LSTM是RNN的一种改进型，特别擅长处理长序列数据，广泛用于语言模型和文本生成。 结构特点：LSTM通过引入三个门（输入门、遗忘门、输出门）来控制信息的流动，有效解决了传统RNN的梯度消失问题。 深度信念网络（DBN） 定义和用途：深度信念网络主要用于无监督学习任务，如特征提取和图像识别。 结构特点：DBN由多层受限玻尔兹曼机（RBM）堆叠而成，每层RBM学习数据在不同抽象层次的表示。 生成对抗网络（GAN） 定义和用途：生成对抗网络在图像生成、风格转换等领域表现出色。 结构特点：GAN由两部分组成——生成器和判别器。生成器生成数据，判别器评估数据。两者相互竞争，共同进步。 自编码器（Autoencoder） 定义和用途：自编码器用于数据降维和特征学习，特别适用于图像重构和降噪。 结构特点：自编码器通过一个编码过程将输入压缩成一个低维表示，然后通过一个解码过程重构输出，使其尽可能接近原始输入。 神经网络在现实世界的应用 神经网络技术在许多领域都有着广泛的应用，以下是一些主要应用领域的详细介绍：\n医疗健康 诊断：神经网络被用于诊断各种疾病，如癌症、糖尿病等，通过分析医学图像或患者数据来辅助医生作出更准确的诊断。 药物开发：利用神经网络分析化合物和生物体的相互作用，加速新药的开发过程。 金融服务 风险管理：神经网络用于预测贷款违约风险、识别欺诈行为，帮助金融机构更有效地管理风险。 量化交易：在股票市场，神经网络可以分析大量历史数据，预测市场趋势，为自动化交易系统提供决策支持。 零售和电子商务 个性化推荐：神经网络通过分析消费者的购买历史、搜索习惯和偏好，提供个性化的商品推荐。 库存管理：通过预测销售趋势和季节性需求变化，神经网络帮助零售商优化库存管理。 自动驾驶和交通管理 环境感知：神经网络处理来自车辆传感器的数据，如摄像头、雷达等，以实现对周围环境的感知。 决策制定：在复杂的交通环境中，神经网络帮助自动驾驶车辆做出快速而准确的驾驶决策。 教育和研究 个性化学习：神经网络分析学生的学习习惯和进度，提供个性化的学习资源和辅导。 科学研究：在科学研究领域，神经网络用于数据分析、模式识别，加速科学发现的过程。 娱乐和媒体 内容创作：在音乐、文本和图像创作领域，神经网络能够生成新的创意内容，如作曲、写作或绘画。 游戏开发：在视频游戏中，神经网络用于创建更真实的游戏环境和更智能的非玩家角色（NPC）。 安全和监控 异常检测：神经网络用于识别网络安全威胁，如恶意软件或入侵尝试。 视频监控：在公共安全领域，神经网络帮助分析监控视频，识别可疑行为或事件。 环境和能源 气候模型：神经网络用于模拟和预测气候变化，帮助科学家更好地理解环境变化。 能源优化：在能源行业，神经网络用于预测能源需求，优化能源分配和利用。 代码实战 import tensorflow as tf from tensorflow.keras.datasets import mnist from tensorflow.keras.models import Sequential from tensorflow.keras.layers import Dense, Conv2D, Flatten, MaxPooling2D from tensorflow.keras.utils import to_categorical import matplotlib.pyplot as plt # 加载数据 (train_images, train_labels), (test_images, test_labels) = mnist.load_data() # 数据预处理 train_images = train_images.reshape((train_images.shape[0], 28, 28, 1)) test_images = test_images.reshape((test_images.shape[0], 28, 28, 1)) # 归一化 train_images = train_images.astype(\u0026#39;float32\u0026#39;) / 255 test_images = test_images.astype(\u0026#39;float32\u0026#39;) / 255 # 标签的one-hot编码 train_labels = to_categorical(train_labels) test_labels = to_categorical(test_labels) # 构建模型 model = Sequential([ Conv2D(32, kernel_size=(3, 3), activation=\u0026#39;relu\u0026#39;, input_shape=(28, 28, 1)), MaxPooling2D(pool_size=(2, 2)), Flatten(), Dense(10, activation=\u0026#39;softmax\u0026#39;) ]) # 编译模型 model.compile(optimizer=\u0026#39;adam\u0026#39;, loss=\u0026#39;categorical_crossentropy\u0026#39;, metrics=[\u0026#39;accuracy\u0026#39;]) # 训练模型 history = model.fit(train_images, train_labels, validation_data=(test_images, test_labels), epochs=5) # 评估模型 test_loss, test_acc = model.evaluate(test_images, test_labels) print(\u0026#39;Test accuracy:\u0026#39;, test_acc) # 绘制训练过程中的准确率和损失值 plt.figure(figsize=(12, 5)) # 绘制准确率变化 plt.subplot(1, 2, 1) plt.plot(history.history[\u0026#39;accuracy\u0026#39;], label=\u0026#39;Training Accuracy\u0026#39;) plt.plot(history.history[\u0026#39;val_accuracy\u0026#39;], label=\u0026#39;Validation Accuracy\u0026#39;) plt.title(\u0026#39;Accuracy over Epochs\u0026#39;) plt.xlabel(\u0026#39;Epoch\u0026#39;) plt.ylabel(\u0026#39;Accuracy\u0026#39;) plt.legend() # 绘制损失值变化 plt.subplot(1, 2, 2) plt.plot(history.history[\u0026#39;loss\u0026#39;], label=\u0026#39;Training Loss\u0026#39;) plt.plot(history.history[\u0026#39;val_loss\u0026#39;], label=\u0026#39;Validation Loss\u0026#39;) plt.title(\u0026#39;Loss over Epochs\u0026#39;) plt.xlabel(\u0026#39;Epoch\u0026#39;) plt.ylabel(\u0026#39;Loss\u0026#39;) plt.legend() plt.show() 总结 神经网络是一种模仿人脑结构和功能的强大计算模型，它在机器学习和人工智能领域占据核心地位。由互联的神经元节点组成，这些节点能够在训练过程中学习并适应各种数据模式。神经网络的多样性体现在其众多类型上，如卷积神经网络（CNN）适用于图像处理，循环神经网络（RNN）和长短期记忆网络（LSTM）优秀于处理序列数据等。它们广泛应用于各个领域，包括医疗诊断、金融服务、自动驾驶、语音识别和自然语言处理等，凭借其对复杂数据模式的高效处理能力，在众多行业中展示出巨大的潜力和价值。此外，神经网络在实际应用中通常涉及大量数据的训练，需要专业的知识进行架构设计和优化，以及合理的方法进行性能评估和调整。随着技术的不断进步，神经网络未来的发展将进一步推动人工智能领域的创新和突破。\n往期推荐 一文看懂机器学习 机器学习-房价预测建模 机器学习 | 基础术语与符号 机器学习 | 特征缩放 机器学习| K 近临(K Nearest-Neighbours) 机器学习| K邻近疾病预测演示 机器学习 | K均值聚类(K-means Clustering) 机器学习 | 朴素贝叶斯原理实战 机器学习 | 线性回归 机器学习 | 支持向量机线性可分 机器学习 | 支持向量机线性不可分 机器学习 | 非线性支持向量机 机器学习 | 自组织映射 欢迎扫码关注公众号，订阅更多文章!\n","permalink":"https://www.devean.cn/post/2024-01-11-machine-learning-neural-networks/","title":"神经网络 | 机器学习"},{"contents":"引言 机器学习领域中，决策树是一种强大的算法，被广泛应用于分类和回归问题。本文将深入探讨决策树的概念、原理、流程、工业应用场景，并通过代码实践展示其实现。\n概念 决策树是一种树状模型，用于对实例进行决策。它的结构类似于流程图，其中每个内部节点代表一个特征或属性，每个分支代表一个决策，而每个叶子节点代表一个类别或输出。通过沿着树的分支进行决策，最终到达叶子节点以得到预测结果。针对“今天是否打高尔夫”这个问题决策树推理过程！\n原理 决策树的构建基于信息论的概念。常用的决策树算法包括ID3、C4.5、CART等，它们通过选择最佳的特征进行节点分裂，以最大化信息增益或基尼指数。\n决策树的组成 根节点（Root Node）： 决策树的起点，通常代表整个数据集。 内部节点（Internal Node）： 非叶节点，用于进一步划分数据。 叶节点（Leaf Node）： 决策树的终端节点，每个叶节点代表一个数据类别或预测值。 决策树的生成 特征选择 从训练数据中选出最佳特征作为当前节点的分裂标准。在决策树模型中，我们有三种方式来选取最优特征，包括信息增益、信息增益率和基尼指数。\n信息增益 信息增益是一种用于特征选择的评估标准，它衡量了通过某一特征对数据进行划分后，数据纯度的提高程度。在决策树生成过程中，选择信息增益最大的特征作为当前节点的分裂标准。信息增益的计算公式为：\n$$G(X, A) = H(X) - \\sum_{i=1}^{m} \\frac{|D_i|}{|D|} H(D_i)$$\n其中：\n$G(X, A)$ 是特征 $A$ 的信息增益； $H(X)$ 是整个数据集的信息熵； $D_i$ 是特征 $A$ 划分后的子数据集； $|D_i|$ 是子数据集的大小； $|D|$ 是整个数据集的大小； $H(D_i)$ 是子数据集 $D_i$ 的信息熵。 信息增益越大，表示选择该特征进行分裂能够带来更大的纯度提升，使得决策更准确。\n信息增益率 增益率是信息增益的一种变体，它对信息增益进行了归一化，解决了信息增益对取值数目较多的特征的偏好问题。增益率的计算公式为：\n$$Gain_Ratio(X, A) = \\frac{G(X, A)}{H(A)}$$\n其中：\n$ Gain_Ratio(X, A) $ 是特征 $ A $ 的增益率； $ H(A) $ 是特征 $ A $ 的信息熵。 增益率不仅考虑了信息增益，还考虑了特征本身的信息熵，避免了对取值数目较多的特征的过度偏好。\n基尼指数 基尼指数是衡量数据不纯度的指标，用于特征选择和节点分裂。在决策树中，选择基尼指数最小的特征进行分裂。基尼指数的计算公式为：\n$$Gini(X) = 1 - \\sum_{i=1}^{n} P(x_i)^2 $$\n其中：\n$ Gini(X) $ 是数据集 $X$ 的基尼指数； $ P(x_i)$ 是第 $ i$ 个类别在总类别中的概率。 基尼指数越小，表示数据越纯净。选择基尼指数最小的特征进行分裂，能够使得决策树更加有效地进行分类。\n综合而言，信息增益、增益率和基尼指数都在决策树中起到了关键的作用，帮助选择最佳的特征进行节点分裂，提高决策树的性能和泛化能力。\n决策树的剪枝 预剪枝 在决策树生成过程中，对每个节点进行评估，若当前节点无法提高模型的泛化能力，则停止生成子节点。\n后剪枝 先生成完整的决策树，然后从下到上对每个非叶节点进行评估，若删除或合并当前节点可以提高模型的泛化能力，则进行剪枝操作。\n决策树的流程 数据准备： 收集并准备训练数据。 特征选择： 根据信息增益或基尼指数选择最佳的特征进行分裂。 节点分裂： 根据选定的特征将节点分裂成子节点。 递归构建： 对子节点递归执行上述步骤，直到满足停止条件。 剪枝： 避免过拟合，对决策树进行剪枝优化。 应用场景 决策树适用于简单而清晰的决策问题，具有易解释性和快速训练的特点，常见应用场景包括：\n金融领域 信用评估：根据客户财务情况判断信用风险。 欺诈检测：识别可能的欺诈交易模式。 医疗领域 疾病分类：根据患者症状和检查结果辅助分类疾病。 治疗方案：根据患者特征推荐治疗方案。 制造业 质量控制：识别影响产品质量的关键因素。 生产优化：优化生产流程，提高效率。 营销和销售 客户分群：根据客户特征实现精准营销。 销售预测：预测不同产品销售情况，指导销售策略。 环境科学 生态系统评估：分析影响生态系统健康的因素。 自然灾害预测：通过观测数据预测自然灾害概率。 代码实战 # 导入必要的库 import pandas as pd import numpy as np from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.linear_model import LogisticRegression from sklearn.metrics import classification_report, confusion_matrix, accuracy_score import matplotlib.pyplot as plt # 加载信用卡欺诈检测数据集 url = \u0026#34;https://storage.googleapis.com/download.tensorflow.org/data/creditcard.csv\u0026#34; df = pd.read_csv(url) # 探索性数据分析 print(df.head()) print(df.info()) print(df[\u0026#39;Class\u0026#39;].value_counts()) # 特征选择 X = df.drop(\u0026#39;Class\u0026#39;, axis=1) y = df[\u0026#39;Class\u0026#39;] # 数据集划分 X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # 数据标准化 scaler = StandardScaler() X_train = scaler.fit_transform(X_train) X_test = scaler.transform(X_test) # 创建并训练逻辑回归模型 model = LogisticRegression() model.fit(X_train, y_train) # 在测试集上进行预测 y_pred = model.predict(X_test) # 模型评估 print(\u0026#34;Confusion Matrix:\\n\u0026#34;, confusion_matrix(y_test, y_pred)) print(\u0026#34;\\nClassification Report:\\n\u0026#34;, classification_report(y_test, y_pred)) print(\u0026#34;\\nAccuracy Score:\u0026#34;, accuracy_score(y_test, y_pred)) # 绘制ROC曲线 from sklearn.metrics import roc_curve, auc fpr, tpr, thresholds = roc_curve(y_test, model.predict_proba(X_test)[:,1]) roc_auc = auc(fpr, tpr) plt.figure(figsize=(8, 6)) plt.plot(fpr, tpr, color=\u0026#39;darkorange\u0026#39;, lw=2, label=\u0026#39;ROC curve (area = {:.2f})\u0026#39;.format(roc_auc)) plt.plot([0, 1], [0, 1], color=\u0026#39;navy\u0026#39;, lw=2, linestyle=\u0026#39;--\u0026#39;) plt.xlabel(\u0026#39;False Positive Rate\u0026#39;) plt.ylabel(\u0026#39;True Positive Rate\u0026#39;) plt.title(\u0026#39;Receiver Operating Characteristic (ROC) Curve\u0026#39;) plt.legend(loc=\u0026#34;lower right\u0026#34;) plt.show() 往期推荐 一文看懂机器学习 机器学习-房价预测建模 机器学习 | 基础术语与符号 机器学习 | 特征缩放 机器学习| K 近临(K Nearest-Neighbours) 机器学习| K邻近疾病预测演示 机器学习 | K均值聚类(K-means Clustering) 机器学习 | 朴素贝叶斯原理实战 机器学习 | 线性回归 机器学习 | 支持向量机线性可分 机器学习 | 支持向量机线性不可分 机器学习 | 非线性支持向量机 机器学习 | 自组织映射 欢迎扫码关注公众号，订阅更多文章!\n","permalink":"https://www.devean.cn/post/2024-01-01-machine-learning-decision-trees/","title":"决策树 | 机器学习"},{"contents":"引言 在机器学习的广阔领域中，自组织映射（Self-Organizing Map，SOM）占据了一席之地。它是一种无监督学习的人工神经网络，用于数据的降维和可视化。今天，我们将深入探讨SOM的概念、定理、原理，并通过Python示例展示其在实际问题中的应用。\n历史背景 SOM的提出源于对大脑皮层处理信息的方式的启发。Kohonen教授通过模拟这种生物学上的信息处理机制，创建了一种能够揭示数据内在结构的神经网络模型。从80年代初到现在，SOM已经在许多领域得到了广泛的应用，如金融分析、生物信息学、图像处理等。\n概念 自组织映射（SOM）是由芬兰教授Teuvo Kohonen在1982年提出的一种无监督学习算法。它通过训练过程自我组织，形成一个拓扑结构，能够将高维数据映射到低维空间（通常是二维），同时保持数据的拓扑关系，这使得它在数据可视化方面特别有用。\n与其他机器学习技术的对比 与传统的监督学习方法如深度学习神经网络和支持向量机等相比，SOM提供了一种不同的视角来处理和理解数据。SOM不依赖于标签数据，更专注于揭示数据的内在关系和结构。此外，与主成分分析（PCA）等其他降维技术相比，SOM保留了数据的非线性关系，这在许多复杂数据集的分析中是非常重要的。\n原理 SOM的基本原理是将输入向量映射到一个二维的网格上。每个网格点（神经元）都有一个与输入空间维度相同的权重向量。通过竞争学习，SOM能够调整这些权重向量，使得相似的输入被映射到相近的神经元上。\n竞争阶段： 对于每个输入向量，找到与之最相似（通常是欧氏距离最小）的神经元。 调整阶段： 调整胜出神经元及其邻域内的权重向量，使它们更靠近输入向量。 流程 初始化： 随机初始化神经元的权重向量。\n竞争： 对于每个输入样本，找到最相似的神经元。\n合作： 确定胜出神经元的邻域。\n适应： 调整胜出神经元及其邻域内神经元的权重。\n重复： 重复步骤2-4，直到网络稳定。\n应用场景 数据可视化：将复杂的高维数据集映射到二维空间，以直观的形式展现数据结构。 聚类分析：自动发现数据中的模式和群组。 特征提取：在降维过程中提取数据的关键特征。 异常检测：识别数据集中的异常或离群点。 实际应用案例 在金融领域，SOM被用于信用评分系统，通过分析客户的历史交易数据来预测其信用风险。在生物信息学中，SOM用于分析和分类复杂的基因表达数据。在图像处理领域，SOM用于图像压缩和特征提取，帮助提高图像识别的效率和准确性。\nPython完整示例 import numpy as np import pandas as pd import seaborn as sns import matplotlib.pyplot as plt from sklearn.datasets import load_iris from sklearn.preprocessing import StandardScaler from minisom import MiniSom # 数据加载 data = load_iris() X = data.data y = data.target # 数据分析 - 可视化 df = pd.DataFrame(X, columns=data.feature_names) sns.pairplot(df) plt.show() # 特征工程 - 标准化 scaler = StandardScaler() X_scaled = scaler.fit_transform(X) # 创建并训练SOM som = MiniSom(x=10, y=10, input_len=4, sigma=1.0, learning_rate=0.5) som.random_weights_init(X_scaled) som.train_random(X_scaled, num_iteration=100) # 定义mapped变量 mapped = np.array([som.winner(d) for d in X_scaled]) # 标记每个样本在SOM网格中的位置 - 优化显示 plt.figure(figsize=(10, 10)) for i, m in enumerate(mapped): plt.plot(m[0]+.5, m[1]+.5, marker=\u0026#39;o\u0026#39;, color=plt.cm.rainbow(y[i] / 3.), markersize=12, alpha=0.5) plt.xticks(range(10)) plt.yticks(range(10)) plt.grid() plt.xlim([0, 10]) plt.ylim([0, 10]) plt.show() 在SOM的结果中，相似的数据点被映射到彼此相邻的位置。通过观察SOM生成的图，我们可以直观地看到数据的聚类情况。数据点的分布和聚类可以揭示数据的内在结构和潜在的模式。\n优化策略 优化SOM的关键在于正确选择网络的大小、学习率和邻域函数。此外，根据具体的应用场景，调整这些参数可以提高SOM的性能和准确性。\n结论 自组织映射提供了一种强大且灵活的方法来分析和解释复杂的高维数据。通过其独特的自学习和自组织能力，SOM可以揭示数据中的潜在结构和模式，为数据科学家和研究人员提供了一个强大的工具。\n虽然SOM在许多领域表现出色，但它在处理非常大规模的数据集时面临挑战。随着技术的发展，未来的SOM可能会集成更先进的算法，以提高其在大数据环境中的性能。\n往期推荐 一文看懂机器学习 机器学习-房价预测建模 机器学习 | 基础术语与符号 机器学习 | 特征缩放 机器学习| K 近临(K Nearest-Neighbours) 机器学习| K邻近疾病预测演示 机器学习 | K均值聚类(K-means Clustering) 机器学习 | 朴素贝叶斯原理实战 机器学习 | 线性回归 机器学习 | 支持向量机线性可分 机器学习 | 支持向量机线性不可分 机器学习 | 非线性支持向量机 欢迎扫码关注公众号，订阅更多文章!\n","permalink":"https://www.devean.cn/post/2023-12-19-machine-learning-self-organnizing-maps/","title":"自组织映射 | 机器学习"},{"contents":" 非线性支持向量机（SVM）是一种强大的监督学习算法，用于解决分类和回归问题。它通过使用核技巧将数据映射到高维空间，从而处理非线性关系。在这篇文章中，我们将探讨非线性 SVM 的工作原理、核函数的作用以及如何在实际中应用非线性 SVM。\n核心概念 线性与非线性 SVM 线性 SVM 在处理线性可分数据时效果显著。然而，当数据集呈非线性分布时，线性 SVM 的性能会受限。非线性 SVM 通过核技巧解决了这个问题。\n核技巧（Kernel Trick） 什么是核技巧？ 核技巧是一种通过转换将低维输入空间映射到高维特征空间的方法，它使得非线性特征组合可以被 SVM 以线性方式处理，核技巧由一些数学工具核函数实现。\n核函数定义 核函数是一个数学函数，能够计算数据点在高维特征空间中的内积，而无需直接计算这些特征。换句话说，它可以让我们在原始特征空间中间接地计算在更高维特征空间的内积。\n核函数的作用 维度映射： 核函数通过隐式地将数据映射到一个更高维的空间，帮助处理数据的非线性特征。 计算简化： 直接在高维空间中处理数据是复杂和计算密集的。核函数通过在原始空间中进行计算来避免这个问题，从而简化了计算过程。 处理非线性问题： 在许多实际问题中，数据集无法用线性方法分割。核函数使 SVM 能够通过在高维空间中寻找线性边界来处理这些非线性问题。 常见的核函数 高斯径向基函数（RBF）核： 最常用的核函数，适合处理没有明显特征模式的复杂数据集。 $$K(\\mathbf{x}_i, \\mathbf{x}_j) = \\exp(-\\gamma ||\\mathbf{x}_i - \\mathbf{x}_j||^2)$$ 其中，$\\gamma$ 是缩放参数。\n多项式核： 用于将数据映射到由原始特征的多项式组成的高维空间。 $$K(\\mathbf{x}_i, \\mathbf{x}_j) = (\\gamma \\mathbf{x}_i \\cdot \\mathbf{x}_j + r)^d$$\n其中，$\\gamma$ 是缩放参数，$d$ 是多项式的度数,$ r $ 是系数项。\nSigmoid 核： 类似于神经网络的激活函数。\n$$K(\\mathbf{x}_i, \\mathbf{x}_j) = anh(\\gamma \\mathbf{x}_i \\cdot \\mathbf{x}_j + r)$$ 其中，$\\gamma$ 是缩放参数，$r$ 是偏置项。\n目标函数引入核函数 对于非线性数据集，目标函数和约束条件与线性不可分 SVM 类似，但使用核技巧在高维空间中寻找最优超平面。\n目标函数:\n$\\max_{\\alpha} \\sum_{i=1}^n \\alpha_i - \\frac{1}{2} \\sum_{i,j=1}^n y_i y_j \\alpha_i \\alpha_j K(\\mathbf{x}_i, \\mathbf{x}_j)$\n约束条件调整:\n$\\sum_{i=1}^n y_i \\alpha_i = 0 \\text{ and } 0 \\leq \\alpha_i \\leq C, \\quad \\forall i$\n构建决策函数 在找到最优超平面后，SVM 使用决策函数来评估新样本属于哪个类别。 $$ f(x)=sign(∑_{i=1}^n​y_i​α_i​K(x_i​,x)+b) $$\n应用实例 支持向量机用于预测股票价格（例如 标普 500ETF，代码为 \u0026lsquo;SPY\u0026rsquo;）的未来 \u0026rsquo;n\u0026rsquo; 天收盘价的机器学习模型。\n# 基础库 import pandas as pd import numpy as np import matplotlib.pyplot as plt from matplotlib.font_manager import FontProperties plt.style.use(\u0026#39;fivethirtyeight\u0026#39;) # 设置中文字体 fontPath = \u0026#39;/System/Library/Fonts/PingFang.ttc\u0026#39; myFont = FontProperties(fname=fontPath) # 预处理 from sklearn.preprocessing import MinMaxScaler from sklearn.pipeline import Pipeline from sklearn.model_selection import train_test_split, TimeSeriesSplit, GridSearchCV # SVM from sklearn.svm import SVR # 忽略警告 import warnings warnings.filterwarnings(\u0026#39;ignore\u0026#39;) # 使用 yfinance 加载数据 import yfinance as yf # 创建变量来预测 \u0026#39;n\u0026#39; 天之后的价格 n = 5 # 从 yfinance 加载 SPY 的数据 df = yf.download(\u0026#39;SPY\u0026#39;, start=\u0026#39;2020-01-01\u0026#39;, end=\u0026#39;2023-12-02\u0026#39;, progress=False) df = df[[\u0026#39;Adj Close\u0026#39;]] # 打印数据的最后 5 行 print(df.tail()) # 创建目标变量 df[\u0026#39;Target\u0026#39;] = df[\u0026#39;Adj Close\u0026#39;].shift(-n) print(df.tail(6)) # 构建预测变量和目标变量 X = df[[\u0026#39;Adj Close\u0026#39;]].values[:-n] y = df[\u0026#39;Target\u0026#39;].values[:-n] # 拆分训练集和测试集 X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0, shuffle=False) # 打印训练集和测试集的大小 print(f\u0026#34;训练集和测试集大小：{len(X_train)}, {len(X_test)}\u0026#34;) # 使用管道进行特征缩放和模型训练 pipe = Pipeline([(\u0026#34;scaler\u0026#34;, MinMaxScaler()), (\u0026#34;regressor\u0026#34;, SVR(kernel=\u0026#39;rbf\u0026#39;, C=1e3, gamma=0.1))]) pipe.fit(X_train, y_train) # 预测测试集 y_pred = pipe.predict(X_test) # 输出模型得分 print(f\u0026#39;训练集准确率：{pipe.score(X_train, y_train):0.4f}\u0026#39;) print(f\u0026#39;测试集准确率：{pipe.score(X_test, y_test):0.4f}\u0026#39;) # 使用时间序列交叉验证 tscv = TimeSeriesSplit(n_splits=5) # 获取参数列表 pipe.get_params() # 网格搜索和拟合模型 param_grid = { \u0026#34;regressor__C\u0026#34;: [0.1, 1, 10, 100, 1000], \u0026#34;regressor__kernel\u0026#34;: [\u0026#34;poly\u0026#34;, \u0026#34;rbf\u0026#34;, \u0026#34;sigmoid\u0026#34;], \u0026#34;regressor__gamma\u0026#34;: [1e-7, 1e-4, 1e-3, 1e-2] } gs = GridSearchCV(pipe, param_grid, n_jobs=-1, cv=tscv, verbose=1) gs.fit(X_train, y_train) # 输出最佳模型得分 print(f\u0026#39;训练集准确率：{gs.score(X_train, y_train):0.6f}\u0026#39;) print(f\u0026#39;测试集准确率：{gs.score(X_test, y_test):0.6f}\u0026#39;) # 创建 DataFrame 来包含关键值 df3 = pd.DataFrame({\u0026#39;X\u0026#39;: X_test.flatten(), \u0026#39;y\u0026#39;: y_pred}) df3[\u0026#39;X\u0026#39;] = df3[\u0026#39;X\u0026#39;].shift(-n) df3[\u0026#39;X-y\u0026#39;] = df3[\u0026#39;X\u0026#39;] - df3[\u0026#39;y\u0026#39;] df3 = df3[:-n] # 检查缺失值 df3.isnull().sum() # 输出均值差异 print(f\u0026#39;均值差异：{np.mean(df3[\u0026#34;X-y\u0026#34;]):0.4f}\u0026#39;) # 可视化预测结果和残差 fig, ax = plt.subplots(2, 2, figsize=(20, 10)) ax[0, 0].scatter(df3[\u0026#39;X\u0026#39;], df3[\u0026#39;y\u0026#39;]) ax[0, 0].set_title(\u0026#39;5天后的价格 vs 预测价格\u0026#39;, fontproperties=myFont) ax[0, 1].plot(df3.index, y_pred[:-n], \u0026#39;crimson\u0026#39;) ax[0, 1].set_title(\u0026#39;预测的价格\u0026#39;, fontproperties=myFont) ax[1, 0].plot(df3.index, df3[\u0026#39;X-y\u0026#39;]) ax[1, 0].set_title(\u0026#39;5天后的价格与预测价格的差异\u0026#39;, fontproperties=myFont) ax[1, 1].hist(df3[\u0026#39;X-y\u0026#39;], bins=50, density=False, color=\u0026#39;orange\u0026#39;) ax[1, 1].set_title(\u0026#39;5天后的价格与预测价格差异的直方图\u0026#39;, fontproperties=myFont) plt.show() 总结 支持向量机（SVM）的目标函数取决于处理的数据类型（线性可分、线性不可分、非线性）和相应的优化策略。以下是这三种情况下 SVM 的目标函数：\n线性可分支持向量机（Hard Margin SVM） 对于线性可分的数据集，目标是找到一个最优的超平面，即最大化两个类别之间的间隔。\n目标函数:$\\min_{\\mathbf{w}, b} \\frac{1}{2} ||\\mathbf{w}||^2$\n约束条件: $y_i (\\mathbf{w} \\cdot \\mathbf{x}_i + b) \\geq 1, \\quad \\forall i$\n线性不可分支持向量机（Soft Margin SVM） 当数据线性不可分时，通过引入“松弛变量”（slack variables）实现间隔和分类违规之间的平衡。\n目标函数: $\\min_{\\mathbf{w}, b, \\xi} \\frac{1}{2} ||\\mathbf{w}||^2 + C \\sum_{i=1}^n \\xi_i$\n约束条件: $y_i (\\mathbf{w} \\cdot \\mathbf{x}_i + b) \\geq 1 - \\xi_i, \\quad \\xi_i \\geq 0, \\quad \\forall i$\n非线性支持向量机 对于非线性数据集，目标函数和约束条件与线性不可分 SVM 类似，但使用核技巧在高维空间中寻找最优超平面。\n目标函数: $\\max_{\\alpha} \\sum_{i=1}^n \\alpha_i - \\frac{1}{2} \\sum_{i,j=1}^n y_i y_j \\alpha_i \\alpha_j K(\\mathbf{x}_i, \\mathbf{x}_j)$\n约束条件: $\\sum_{i=1}^n y_i \\alpha_i = 0 \\text{ and } 0 \\leq \\alpha_i \\leq C, \\quad \\forall i$ \u0026quot;\u0026quot;\u0026quot;\n非线性 SVM 的优势与局限性 总结非线性 SVM 在处理复杂数据集时的优势，以及其面临的挑战和局限性。\n往期推荐 一文看懂机器学习 机器学习-房价预测建模 机器学习 | 基础术语与符号 机器学习 | 特征缩放 机器学习| K 近临(K Nearest-Neighbours) 机器学习| K邻近疾病预测演示 机器学习 | K均值聚类(K-means Clustering) 机器学习 | 朴素贝叶斯原理实战 机器学习 | 线性回归 机器学习 | 支持向量机线性可分 机器学习 | 支持向量机线性不可分 欢迎扫码关注公众号，订阅更多文章!\n","permalink":"https://www.devean.cn/post/2023-12-02-machine-learning-nonlinear-support-vector-machines/","title":"机器学习 | 非线性支持向量机"},{"contents":" 本文从支持向量机线性不可分、软间隔、松弛变量、目标函数、约束条件、超参数 C,实际应用场景判别线性是否可分等几方面讲概述了支持向量机线性不可分\n线性不可分 SVM 有些时候数据本身存在噪点或异常值、在这种场景下,支持向量机又会如何处理呢,看下图\n在这里，我们在红球的边界中有一个蓝球。那么 SVM 是如何对数据进行分类的呢？这很简单！红色球边界中的蓝色球是蓝色球的异常值。SVM 算法具有忽略异常值并找到使边际最大化的最佳超平面的特点。SVM 对异常值具有鲁棒性。\n在这种类型的数据点中，SVM 所做的就是像之前的数据集一样找到最大边距，并在每次点跨越边距时添加惩罚。因此，此类情况下的边距称为软边距。当数据集存在软边距时，SVM 会尝试最小化(1/margin+∧(Σpenalty))。\n引入松弛变量 首先，引入松弛变量（Slack Variables），这些变量表示数据点到超平面的距离。对于每个数据点，引入一个对应的松弛变量 $\\xi_i$，表示第 $i$ 个数据点允许的错误。\n目标函数修改 软间隔的目标函数通过调整传统硬间隔的目标函数，以考虑错误和松弛变量。新的目标函数可以表示为：\n$ \\min_{w, b, \\xi} \\frac{1}{2}||w||^2 + C \\sum_{i=1}^{N} \\xi_i $\n其中：\n$||w||^2$ 表示模型复杂度，即超平面的法向量的范数。 $\\sum_{i=1}^{N} \\xi_i$ 表示所有松弛变量的总和。 $C$ 是一个用户定义的超参数，用于平衡最小化模型复杂度和最小化分类错误的目标。\n约束条件调整 随着引入了松弛变量，约束条件也需要相应的调整。约束条件现在变为：\n$ y_i(w \\cdot x_i + b) \\geq 1 - \\xi_i, \\quad \\text{for } i = 1, 2, \\ldots, N $\n这确保了即使数据点落在错误的一侧，它们的函数间隔仍然至少为 $1 - \\xi_i$。\n超参数调整 软间隔的效果受到超参数 $C$ 的影响， $C$ 的选择取决于对模型性能的需求。较大的 $C$ 值会更强调正确分类，但可能导致过拟合，而较小的 $C$ 值会更注重找到更大间隔，但可能容忍更多的错误分类。通过在实际问题中调整 $C$ 的值，可以根据具体情况平衡模型的复杂性和对错误的容忍度。这种灵活性使得软间隔成为处理线性不可分数据的有力工具。\n实际应用 在实际数据集上应用软间隔 SVM 时，通常需要通过试验来确定最佳的超参数 $C$。这可能需要使用网格搜索或随机搜索等技术。 软间隔 SVM 特别适用于那些数据本身就包含噪声或异常值的情况。它通过牺牲一些训练准确性，增加了模型对未见数据的泛化能力。\n判断是否完全线性可分 可视化数据：如果数据维度较低（如二维或三维），可以通过可视化来初步判断数据是否可能线性可分。\n尝试线性模型：在不考虑松弛变量的情况下，使用线性 SVM 或其他线性模型对数据进行拟合。如果模型表现良好，这可能表明数据是线性可分的。\n评估模型性能：使用交叉验证等方法来评估线性模型的性能。如果线性模型的性能不佳，可能意味着数据不是完全线性可分的。\n分析误差类型：查看模型的误差类型，如是否存在系统性误差，这可能表明数据结构的非线性特性。\n决定是否引入参数 C 处理不完全线性可分的数据：如果数据不是完全线性可分的，引入 C 是必要的。这有助于控制模型对于误分类的惩罚强度。\n防止过拟合：参数 C 可以帮助控制模型的复杂度，从而防止过拟合。特别是在数据量不是很大的情况下，合适的 C 值尤为重要。\n模型调优：通过网格搜索、随机搜索或基于模型的搜索方法（如贝叶斯优化）来找到最优的 C 值。这通常是通过交叉验证完成的。\n实验和验证：不同的 C 值可能会导致模型性能显著不同。实验不同的 C 值，并通过验证集或测试集来评估模型性能。\n平衡偏差和方差：选择 C 值是平衡模型偏差和方差的一个重要步骤。较小的 C 值可能导致高偏差（欠拟合），而较大的 C 值可能导致高方差（过拟合）。\n实战 数据加载预处理 # 导入所需的库 import numpy as np import matplotlib.pyplot as plt from sklearn.datasets import make_moons from sklearn.preprocessing import StandardScaler from sklearn.svm import SVC from sklearn.model_selection import train_test_split, GridSearchCV from sklearn.metrics import classification_report, confusion_matrix import seaborn as sns # 1. 数据加载 X, y = make_moons(n_samples=300, noise=0.2, random_state=42) # 2. 特征工程 - 标准化处理 scaler = StandardScaler() X_scaled = scaler.fit_transform(X) # 3. 探索性数据分析（EDA） plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.RdYlBu) plt.title(\u0026#34;EDA - Moon Dataset\u0026#34;) plt.xlabel(\u0026#34;Feature 1\u0026#34;) plt.ylabel(\u0026#34;Feature 2\u0026#34;) plt.show() # 4. 模型训练和评估 # 划分训练集和测试集 X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42) # 创建 SVM 模型 model = SVC(kernel=\u0026#39;rbf\u0026#39;, C=1, gamma=1) # 训练模型 model.fit(X_train, y_train) # 预测和评估 y_pred = model.predict(X_test) print(classification_report(y_test, y_pred)) # 5. 参数调优 - 使用网格搜索 param_grid = {\u0026#39;C\u0026#39;: [0.1, 1, 10, 100], \u0026#39;gamma\u0026#39;: [1, 0.1, 0.01, 0.001]} grid = GridSearchCV(SVC(), param_grid, refit=True, verbose=2) grid.fit(X_train, y_train) # 输出最佳参数 print(\u0026#34;Best Parameters:\u0026#34;, grid.best_params_) # 6. 可视化模型性能 # 绘制混淆矩阵 cm = confusion_matrix(y_test, grid.predict(X_test)) sns.heatmap(cm, annot=True, fmt=\u0026#39;g\u0026#39;) plt.title(\u0026#34;Confusion Matrix\u0026#34;) plt.xlabel(\u0026#34;Predicted label\u0026#34;) plt.ylabel(\u0026#34;True label\u0026#34;) plt.show() 往期推荐 一文看懂机器学习 机器学习-房价预测建模 机器学习 | 基础术语与符号 机器学习 | 特征缩放 机器学习| K 近临(K Nearest-Neighbours) 机器学习| K邻近疾病预测演示 机器学习 | K均值聚类(K-means Clustering) 机器学习 | 朴素贝叶斯原理实战 机器学习 | 线性回归 机器学习 | 支持向量机线性可分 欢迎扫码关注公众号，订阅更多文章!\n","permalink":"https://www.devean.cn/post/2023-11-19-machine-learning-support-vector-machine-non-linearly-separable/","title":"机器学习 | 支持向量机线性不可分"},{"contents":" 本文从支持向量机概念、硬间隔、软间隔和非线性的区别、原理、术语、最大间隔数学推导几个方面详细讲解线性可分的支持向量机。\n基础概念 支持向量机(Support Vector Machine)SVM，是一种监督学习模型，适用于二分类任务。SVM 算法的主要目标是在 N 维空间中找到能够将特征空间中不同类的数据点分开的最优超平面。超平面尝试使不同类的最近点之间的间隔尽可能最大。超平面的维度取决于特征的数量。如果输入特征的数量是两个，那么超平面只是一条线。如果输入特征的数量为三个，则超平面变为二维平面。当特征数量超过三个时就变得难以想象。\n硬间隔、软间隔和非线性SVM区别 硬间隔数据是完全准确可分的、不存在分类错误的情况，软间隔是允许一定量的样本分类错误、而线性不可分是线性公式无解，只能使用非线性方式求解的\n线性可分 SVM 即假设样本数据是线性可分离的情况下，我们直接使用线性 SVM 对数据进行分类。基本原理是找到一个超平面，将数据划分为两个类，使得类别之间的间隔最大化。\n定义 给定训练样本集 D={(x1,y1)，(x2,y2)，\u0026hellip;，(xm,ym)},yi∈{-1,+1}，分类学习最基本的想法就是基于训练集 D 在样本空间中找到一个划分超平面，将不同类别的样本分开。下面列子以二维数据展开、实际生活中多维数据与二维几乎无差别，只是数据特征维度变为了多维。\n直观上看，应该去找位于两类训练样本“正中间”的划分超平面，即图中红色的那个，因为该划分超平面对训练样本局部扰动的“容忍”性最好。如果，由于训练集的局限性或噪声的因素，训练集外的样本可能比图中的训练样本更接近两个类的分隔界，这将使许多划分超平面出现错误，而红色的超平面受影响最小。\n要找到最佳超平面,即得找到样本数据点离超平面最近的点的距离最大化，从上面两个图中可以看出,离图中红线最近的点即被圈住的样本到红色超平面距离最大。其中 w=(w1;w2;\u0026hellip;;wd)为法向量，决定了超平面的方向；b 为位移项，决定了超平面与原点之间的距离。\n术语 向量距离：即上图中任何一个样本点到红色超平面的距离 $$r= \\frac {w^Tx+b} {||w||}$$\n支持向量（support vector）：即图中圈中的样本,支持向量是距离超平面最近的数据点，在决定超平面和边距方面起着关键作用。\n决策边界:即上图中红色超平面,用于分隔特征空间中不同类的数据点。在线性分类的情况下，它将是一个线性方程 $$w^Tx+b=0$$\n上边界：将超平面放大 n 背后\n$$w^Tx+b=1$$\n下边界： $$w^Tx+b=-1$$\n间隔(margin)：即如图上下边界之间的距离\n$$\\lambda=\\frac {2} {||w||}$$\n求解线性 SVM 决策超平面 1. 列出知超平面方程组 $$w^Tx+b=0$$ $$w^Tx+b=1$$ $$w^Tx+b=-1$$\n2 假设正负超平面向量 假设正决策超平面上的存在点 $x_m$、负决策超平面上存在点 $x_n$, 求两点之间的向量可如下图\n3.可将正负超平面上的向量带入方程计算\n$$\\vec w_m \\vec x+b=1$$ $$\\vec w_n \\vec x+b=-1$$\n$$\\vec {w} \\cdot (\\vec x_m- \\vec x_n)=2$$\n4.在决策超平面上假设存在两点 $x_p,x_q$\n$$\\vec w_p \\vec x+b=0$$ $$\\vec w_q \\vec x+b=0$$\n$$\\vec {w} \\cdot (\\vec x_p- \\vec x_q)=0$$\n5. 由此 我们可推出向量 $\\vec w$ 与超平面垂直，即为超平面的法向量\n6.基于向量定理计算 根据已知,及上图结论，我们可推导出 $$\\vec {w} \\cdot (\\vec x_m- \\vec x_n)=2$$ $$||\\vec x_m- \\vec x_n|| * cos \\theta * ||\\vec {w} || =2$$\n7. 推导间隔 L 从上图中可以看出向量 $\\vec x_m- \\vec x_n$ 投影到法向量 $\\vec w$ 上，就等于间隔 L\n$$||\\vec x_m- \\vec x_n|| * cos \\theta =L$$\n$$L * ||\\vec {w} || =2$$\n$$L = \\frac {2} {||\\vec {w} ||}$$\n8.定义问题 我们的目标是最大化分类间隔，即最大化 $\\frac{2}{|w|}$。等价地，我们最小化 $\\frac{1}{2} |w|^2$。优化问题可以写成：\n最小化:\n$$ \\frac{1}{2} |w|^2 $$\n约束条件:\n$$ y_i(w \\cdot x_i + b) \\geq 1 \\quad \\text{for all } i = 1, 2, \\ldots, m $$\n9. 引入拉格朗日乘子：\n引入拉格朗日乘子 $\\alpha_i \\geq 0$，定义拉格朗日函数：\n拉格朗日方程:\n$$ L(w, b, \\alpha) = \\frac{1}{2} |w|^2 - \\sum_{i=1}^{m} \\alpha_i [y_i(w \\cdot x_i + b) - 1] $$\n10. 求解对偶问题：\n通过对拉格朗日函数对 $w$ 和 $b$ 求偏导数，并令其等于零，我们得到：\n偏导数与置换:\n$$ w = \\sum_{i=1}^{m} \\alpha_i y_i x_i $$\n对偶问题:\n$$ \\text{maximize} \\quad W(\\alpha) = \\sum_{i=1}^{m} \\alpha_i - \\frac{1}{2} \\sum_{i,j=1}^{m} \\alpha_i \\alpha_j y_i y_j x_i \\cdot x_j $$\n$$ \\text{subject to} \\quad \\alpha_i \\geq 0, \\quad \\sum_{i=1}^{m} \\alpha_i y_i = 0 $$\n11. 最大化间隔的数学表达：\n通过对偶问题的求解，得到一组优化的拉格朗日乘子 $\\alpha^*$。最大化分类间隔的数学表达式为：\n最大间隔:\n$$ \\text{maximize} \\quad \\frac{2}{|w|} = \\frac{2}{\\sqrt{\\sum_{i=1}^{m} (\\alpha_i^* y_i x_i)^2}} $$\n12. 计算最大间隔：\n最大间隔的计算是通过对偶问题中的 $\\alpha^$ 计算得到的。具体地，最大间隔是 $\\frac{2}{|w|}$，其中 $w$ 由 $\\sum_{i=1}^{m} \\alpha_i^ y_i x_i$ 计算得到。\n欢迎大家关注 往期推荐 一文看懂机器学习\n机器学习-房价预测建模\n机器学习 | 基础术语与符号\n机器学习 | 特征缩放\n机器学习| K 近临(K Nearest-Neighbours)\n机器学习| K邻近疾病预测演示\n机器学习 | K均值聚类(K-means Clustering)\n机器学习 | 朴素贝叶斯原理实战\n机器学习 | 线性回归\n","permalink":"https://www.devean.cn/post/2023-11-18-machine-learning-support-vector-machine-linearly-separable/","title":"机器学习 | 支持向量机线性可分"},{"contents":" 本文简单科普了什么是套利、有哪些套利方法、套利失效的原因。\n什么是套利 套利是指在无风险回报率之外获得确定的利润。 用量化金融的语言来说，我们 可以说套利机会是今天价值为零但未来价值为正的投资组合 的概率，并且未来为负值的概率为零。\n市场上不存在套利机会的假设是经典金融理论的基础。这个想法是俗话说“天下没有免费的午餐”。\n现在我们可以看到，我们可以想到几种套利方式。以下是最重要的几种套利的列表和说明。\n静态套利是一种无需重新平衡正向持仓的套利。 动态套利是一种需要在未来进行交易的价差套利策略，通常取决于市场状况。 统计套利并非套利，而只是根据过去的统计数据预测，可能获得超过无风险收益的利润（甚至可能根据承担的风险进行适当调整）。 模型无关套利是一种不依赖于任何金融工具数学模型而发挥作用的套利。例如，可利用的对看跌期权平价的违反或对债券与掉期之间关系的违反 依赖于模型的套利需要一个模型。例如，由于不正确的波动率估计，期权定价错误。为了从套利中获利，你需要delta对冲，这需要一个模型。 这有几个套利失效的原因\n报价错误或不可交易 期权价格和股票价格没有同步报价 存在您未计算在内的竞价价差 你的模型是错误的，或者存在你没有考虑到的风险因素 What Is Arbitrage Arbitrage is making a sure profit in excess of the risk-free rate of return. In the language of quantitative finance we can say that an arbitrage opportunity is a portfolio of zero value today which is of positive value in the future with positive probability,and of negative value in the future with zero probability.\nThe assumption that there are no arbitrage opportunities in the market is fundamental to classical finance theory.This idea is popularly known as \u0026rsquo;there\u0026rsquo;s no such thing as a free lunch.\nNow we can see that there are several types of arbitrage that we can think of. Here is a list and description of the most important.\nA static arbitrage is an arbitrage that does not require re-balancing of positives A dynamic arbitrage is an arbitrage that requires trading instruments in the future,generally contingent on market states A statistical arbitrage is not an arbitrage that but simply a likely profit in excess of the risk-free return (perhaps even suitably adjusted for risk taken) as predicted by past statistics Model-independent arbitrage is an arbitrage which dose not depend on any mathematical model of financial instruments to work. For example, an exploitable violation of put-call parity or a violation of the relationship between bonds and swaps Model-dependent arbitrage dose require a model. For example, options mis-priced because of incorrect volatility estimate. To profit from the arbitrage you need to delta hedge, and that requires a model. Here are several reasons for arbitrage fails\nQuoted prices are wrong or not tradeable Option and stock prices were not quoted synchronously There is a bid-off spread you have not accounted for Your model is wrong, or there is a risk factor you have not accounted for [1] Wilmott，M 2009 Frequently Asked Questions In Quantitative Finance, second edition. John Wiley \u0026amp; Sons,Ltd.\n欢迎扫码关注公众号，订阅更多文章!\n","permalink":"https://www.devean.cn/post/2023-11-15-what-is-arbitrage/","title":"什么是套利 | 翻译"},{"contents":" 本文量从量化金融中数学的应用、常见的建模方法、常见的数学工具普及了量化金融中的应用。\n量化金融中有哪些不同类型的数学？ 在数量金融中使用最多的数学领域是概率论和微分方程。当然，通常需要数值方法来生成数字。\n这里列出了各种建模方法和一些有用的工具。“建模方法”和“工具”之间的区别将开始变得清晰。\n建模方法\n概率模型 决策模型 离散：差分方程 连续：微分方程 实用工具\n模拟 离散化方法 近似值 渐进分析 级数解 格林函数 虽然这些清单并非完全是任意列出的，但可以接受一些批评或补充。让我们先来看看建模方法。\nWhat are the different types of Mathematics found in quantitative finance? The field of mathematics most used in quantitative finance are those of probability theory and differential equations. And,of course, numerical methods are usually needed for producing numbers.\nHere‘s a list of the various approaches to modelling and a selection of useful tools. The distinction between a \u0026lsquo;modelling approach\u0026rsquo; and a \u0026rsquo;tool\u0026rsquo; will start to become clear.\nModelling approaches: Probabilistic Deterministic Discrete: difference equations Continuous: differential equations Useful tools Simulations Discretization methods Approximations Asymptotic analysis Series solutions Green\u0026rsquo;s functions While these are not exactly arbitrary lists, they are certainly open to some criticism or addition. Let\u0026rsquo;s first take a look at the modelling approaches.\n[1] Wilmott，M 2009 Frequently Asked Questions In Quantitative Finance, second edition. John Wiley \u0026amp; Sons,Ltd.\n","permalink":"https://www.devean.cn/post/2023-11-14-mathematical-applications-in-quantitative-finance/","title":"量化金融中的数学应用 | 翻译"},{"contents":" 本文从特征缩放概念、目的、常用特征缩放方法：最小-最大缩放、标准缩放、鲁棒缩放、L2 Normalization、L1 Normalization、Power Transformer的公式讲解、Python缩放数据可视化对比诠释了特征缩放\n什么是特征缩放 特征缩放又称归一化，是机器学习中的一种技术，涉及调整数值数据的量度，使所有数据点在相似的尺度上。例如：身高、体重、年龄、收入等个人特征数据，每个维度的区间不一样，为保证所有维度的特征数据尺度一样，我们就需要对原始数据做特征缩放，将身高、体重、年龄、收入都转化为区间[0,1]之间的数据。\n为什么要做特征缩放 收敛速度：梯度下降等迭代方法在各特征尺度一致时会更快地收敛。 避免数值不稳定性：在某些算法中，如果特征的尺度差异很大，可能会导致数值计算问题。 更好的模型解释性：当所有特征都在同一个尺度上，它们的权重可以更容易地相互比较。 目的 使数据均匀：数据缩放通过将数据转换到新的尺度上，使不同特征间的数值大小差异减小。 提高算法性能：缩放可以加快梯度下降的收敛速度，并提高算法（如支持向量机和 K 近邻算法）的性能。 特征缩放方法 最小-最大缩放 (Min-Max Scaling)\n公式: $X_{norm} =\\frac {X - X_{min}} {X_{max} - X_{min}}$ 描述: 将数据缩放到[0,1]范围内的技术。 场景: 当数据分布不是高度偏斜，并且不包含极端值时。 标准化 (Standardization)\n公式: $X_{standard} =\\frac {(X - μ)} σ$ 描述: 通过使数据的平均值为 0，标准差为 1 来缩放数据。 场景：当算法需要数据的标准差为 1，且偏差很小时。 稳健缩放 (Robust Scaling)\n公式: $X_{robust} =\\frac {X - Q1} {Q3 - Q1}$ 描述: 缩放技术，可以减少离群值的影响。 场景：当数据包含许多离群值或异常值时。 L2 Normalization (欧几里得范数)\n公式: $X_{l2} =\\frac {X - μ} {||X||_2}$ 描述: 通过使特征向量的欧几里得长度为 1 来缩放特征。 场景：在图像处理和文本分类中，当数据的方向比其大小更重要时。 L1 Normalization\n公式: $X_{l1} =\\frac {X - μ} {||X||_1}$ 描述: 通过使特征向量的欧几里得长度为 1 来缩放特征。 场景：在图像处理和文本分类中，当数据的方向比其大小更重要时。 Power Transformer a. Box-Cox Transformation\n公式: $X_{Box-Cox} =\\frac {X^{\\lambda}-1} {\\lambda},for \\quad \\lambda \\neq 0,ln(X) \\quad for \\quad \\lambda=0$ 描述: Box-Cox 转换只能应用于正值的数据。它的目标是对非常数方差和非正态分布的数据进行变换，使其更接近正态分布。 场景： 线性模型：在回归、ANOVA 或设计实验中，当我们希望满足线性模型的正态分布假设时。 时间序列分析：稳定化时间序列数据的方差。 方差稳定化：在很多统计模型中，稳定的方差是关键。Box-Cox 转换能够稳定化方差，使其不随因变量的值而变化。 处理倾斜数据：对于正偏斜或负偏斜的数据，Box-Cox 转换可以帮助减少偏斜。 b. Yeo-Johnson Transformation\n公式： $X_{yeo-Johnson}=\\frac {(X+1)^{\\lambda}-1} {\\lambda} \\quad for \\quad \\lambda \\neq 0 \\quad and \\quad X \\geq 0,ln(X+1) \\quad for \\quad\\lambda=0 \\quad and \\quad X \\geq 0，-\\frac {(-X+1)^{\\lambda}-1} {\\lambda} \\quad for \\quad \\lambda \\neq 0 \\quad and \\quad X \u0026lt; 0, -ln(-X+1) \\quad for \\quad \\lambda=0 \\quad and \\quad X \u0026lt;0$ 描述: Yeo-Johnson 转换是 Box-Cox 的扩展，它可以应用于正值、负值和零的数据。这种转换同样旨在使数据更接近正态分布。 场景： 广义线性模型：当我们需要满足广义线性模型的正态分布假设时。 包含零或负值的数据：与 Box-Cox 不同，Yeo-Johnson 转换可以应用于包含零或负值的数据。 方差稳定化：与 Box-Cox 类似，Yeo-Johnson 转换也可以用来稳定化方差。 处理倾斜数据：对于正偏斜或负偏斜的数据，Yeo-Johnson 转换也是一个有效的工具。 数据缩放对比 原始数据 ID 身高(cm) 体重(kg) 心率(bpm) 胆固醇(mg/dL) 年龄 脚长(cm) 1 170 68 75 180 25 25 2 160 50 80 200 30 23 3 180 77 72 220 28 27 4 175 65 78 210 32 26 5 166 58 82 190 29 24 python 代码 import pandas as pd import numpy as np import matplotlib.pyplot as plt from sklearn.preprocessing import (MinMaxScaler, StandardScaler, RobustScaler, MaxAbsScaler, QuantileTransformer, PowerTransformer) from matplotlib.font_manager import FontProperties # 设置中文字体路径 myFont = FontProperties(fname=\u0026#39;/System/Library/Fonts/PingFang.ttc\u0026#39;) # Create the dataset data = { \u0026#39;身高 (cm)\u0026#39;: [170, 175, 168, 180, 172], \u0026#39;体重 (kg)\u0026#39;: [65, 72, 58, 80, 68], \u0026#39;年龄\u0026#39;: [25, 30, 28, 35, 29], \u0026#39;脚长 (cm)\u0026#39;: [25, 26, 25, 27, 26], \u0026#39;收缩压 (mmHg)\u0026#39;: [120, 125, 118, 128, 121], \u0026#39;胆固醇 (mg/dL)\u0026#39;: [190, 200, 185, 210, 195], \u0026#39;心率 (bpm)\u0026#39;: [70, 72, 68, 75, 71] } df = pd.DataFrame(data) # Define a unique color for each feature colors = plt.cm.Accent(np.linspace(0, 1, len(df.columns))) # Scaling methods scalars = { \u0026#39;原始数据\u0026#39;: None, \u0026#39;最小-最大 缩放器\u0026#39;: MinMaxScaler(), \u0026#39;标准缩放器\u0026#39;: StandardScaler(), \u0026#39;鲁棒缩放器\u0026#39;: RobustScaler(), \u0026#39;最大绝对值缩放器\u0026#39;: MaxAbsScaler(), \u0026#39;分位数转换器\u0026#39;: QuantileTransformer(n_quantiles=5), \u0026#39;幂转换器\u0026#39;: PowerTransformer(method=\u0026#39;yeo-johnson\u0026#39;) } # Determine the number of rows and columns for the subplots n_features = len(df.columns) n_scalars = len(scalars) n_rows = n_features n_cols = n_scalars fig, axes = plt.subplots(nrows=n_rows, ncols=n_cols, figsize=(20, 4 * n_features)) for row, feature in enumerate(df.columns): for col, (name, scaler) in enumerate(scalars.items()): ax = axes[row, col] if scaler: scaled_data = scaler.fit_transform(df[[feature]]) else: scaled_data = df[[feature]].values ax.hist(scaled_data, bins=10, color=colors[row], edgecolor=\u0026#39;black\u0026#39;) if row == 0: ax.set_title(name, fontproperties=myFont) if col == 0: ax.set_ylabel(feature, fontproperties=myFont, rotation=0, labelpad=60, ha=\u0026#39;right\u0026#39;) fig.text(0.5, 0.01, \u0026#39;Value\u0026#39;, ha=\u0026#39;center\u0026#39;, fontproperties=myFont) fig.text(0.01, 0.5, \u0026#39;Frequency\u0026#39;, va=\u0026#39;center\u0026#39;, rotation=\u0026#39;vertical\u0026#39;, fontproperties=myFont) # Adjust layout plt.tight_layout(pad=1.0) plt.subplots_adjust(top=0.90,left=0.12, wspace=0.4, hspace=0.5,bottom=0.08) plt.suptitle(\u0026#34;各特征在不同缩放方法下的分布\u0026#34;, fontproperties=myFont) plt.show() 特征缩放图示 欢迎扫码关注公众号，订阅更多文章!\n","permalink":"https://www.devean.cn/post/2023-11-11-machine-learning-feature-scaling/","title":"机器学习 | 特征缩放"},{"contents":" 本文从概念、原理、距离函数、K 值选择、K 值影响、、优缺点、应用几方面详细讲述了 KNN 算法。\nK 近临(K Nearest-Neighbours) 一种简单的监督学习算法，惰性学习算法，在技术上并不训练模型来预测。适用于分类和回归任务。它的核心思想是：相似的对象彼此接近。例如，若果你想分类一个新的数据点(绿点)，可以查看训练数据中哪些数据点与它最接近，并根据这些最接近的数据点和标签来预测它的标签(红点或蓝圆)。\n概念 K: 这是一个用户指定的正整数，即训练数据分类数量，代表要考虑的最近邻居的数量，上图中假设 K=2,即训练数据分类为蓝色圆和红色三角两类标签。\n距离函数: 用于计算数据点之间的距离。最常见的是欧几里得距离、曼哈顿距离、马氏距离等。\n投票机制:\n分类任务: 将根据 k 个最近邻的多数投票来确定新数据点的类别。 回归任务: 通常取 k 个最近邻的输出变量的平均值。 原理 距离计算： 对于给定的新数据点，计算它与训练数据集中每个点的距离。 选取 K 个邻居： 从训练数据集中选取距离最近的 K 个点。 投票 (对于分类)： 对于 K 个邻居，看哪个类别最为常见，并将其指定为新数据点的类别。 均值 (对于回归)： 对于 K 个邻居，计算其属性的平均值，并将其指定为新数据点的值。 距离度量 欧几里得距离 (Euclidean Distance) 欧几里得距离的名称来源于古希腊数学家欧几里得，是衡量两点在平面或更高维空间中的\u0026quot;直线\u0026quot;距离。它基于勾股定理，用于计算两点之间的最短距离。在日常生活中，我们经常无意识地使用欧几里得距离，例如，当我们说两地之间的\u0026quot;直线\u0026quot;距离时，实际上是在引用欧几里得距离。 公式: 给定两点 P 和 Q，其坐标分别为 $P(x_1, x_2, \u0026hellip;, x_n)$ 和 $Q(y_1, y_2, \u0026hellip;, y_n)$ 在一个 n 维空间中，它们之间的欧几里得距离 d 定义为：\n$d(P, Q) = \\sqrt{\\sum_{i=1}^{n} (x_i - y_i)^2}$\n曼哈顿距离 (Manhattan Distance) 曼哈顿距离得名于纽约的曼哈顿，因为在曼哈顿的街道布局是网格状的。想象一下，你在一个街区的一个角落，要走到对面的角落，你不能直接穿越街区，只能沿着街道走。这就是曼哈顿距离的来源，也因此它有时被称为“城市街区距离”。\n公式\n给定两点 P 和 Q，其坐标分别为 $P(x_1, x_2, \u0026hellip;, x_n)$ 和 $Q(y_1, y_2, \u0026hellip;, y_n)$ 在一个 n 维空间中，它们之间的曼哈顿距离 L1 定义为：\n$L1(P, Q) = \\sum_{i=1}^{n} |x_i - y_i|$\n闵可夫斯基距离 (Minkowski Distance) 闵可夫斯基距离是一种在向量空间中度量两个点之间距离的方法。它实际上是一种泛化的距离度量，可以看作是其他距离度量（如欧几里得距离、曼哈顿距离）的泛化。通过改变一个参数p，它可以表示多种距离度量。\n公式\n给定两点 P 和 Q，其坐标分别为 $P(x_1, x_2, \u0026hellip;, x_n)$ 和 $Q(y_1, y_2, \u0026hellip;, y_n)$ 在一个 n 维空间中，它们之间的闵可夫斯基距离 Lp 定义为：\n$Lp(P, Q) = \\left( \\sum_{i=1}^{n} |x_i - y_i|^p \\right)^{\\frac{1}{p}}$\n其中 p 是一个大于等于 1 的实数。特定的 p 值会导致其他常见的距离度量：\n当 p = 1 时，这变成了曼哈顿距离。 当 p = 2 时，这变成了欧几里得距离。 余弦相似性 (Cosine Similarity) 余弦相似性度量了两个向量方向的相似度，而不是它们的大小。换句话说，它是通过比较两个向量之间的夹角来测量它们的相似性的。夹角越小，相似性就越高。\n它经常在高维空间中（如 TF-IDF 权重的文档向量）使用，因为在高维空间中，基于欧几里得距离的相似性度量可能不太有效。\n公式 给定两个向量 A 和 B，它们的余弦相似性定义为：\n$\\text{cosine similarity}(A, B) = \\frac{A \\cdot B}{|A| |B|}$\n其中：\n$A \\cdot B$ 是向量 A 和 B 的点积。 $|A|$ 和 $|B|$ 分别是向量 A 和 B 的欧几里得长度（或模）。 公式可以进一步扩展为：\n$\\text{cosine similarity}(A, B) = \\frac{\\sum_{i=1}^{n} A_i B_i}{\\sqrt{\\sum_{i=1}^{n} A_i^2} \\sqrt{\\sum_{i=1}^{n} B_i^2}}$\n这里，n 是向量的维度，而 $A_i$ 和 $B_i$ 分别是向量 A 和 B 在第 i 维度上的值。\n余弦相似性值范围为[-1, 1]，其中 1 表示完全相似，0 表示不相关，而-1 表示完全相异。\nK 值的确定方法： 交叉验证： 这是确定 k 值的最常用方法。对于每一个可能的 k 值，使用交叉验证计算模型的预测错误率，选择错误率最低的 k 值。\n启发式方法： 有时，可以选择 sqrt(n)作为起始点，其中 n 是训练样本的数量。这只是一个粗略的估计，通常需要进一步验证。\n误差曲线： 画出不同 k 值对应的误差率曲线，选择误差变化开始平稳的点。\n领域知识： 在某些应用中，基于领域知识和经验选择 k 值可能更为合适。\nK 值的影响： 过小的 k 值： 分类：模型可能变得过于敏感和复杂。它可能对训练数据中的噪声或异常点特别敏感，从而容易过拟合。当 k=1 时，任何训练数据中的异常点都可能影响预测结果。 回归：模型可能会受到异常值的强烈影响，导致预测结果出现明显的波动。 过大的 k 值： 分类：模型可能变得过于简化。随着 k 值的增加，分类决策的边界会变得更加平滑，可能会忽视数据中的细微模式，导致欠拟合。 回归：模型同样可能会过于简化。大 k 值使模型的预测偏向于所有数据点的平均值，因此可能会忽视数据中的局部特性或细节。 优缺点 优点 简单且直观。 无需训练阶段，适用于动态变化的数据集。 对异常值不敏感（取决于 K 的大小）。\n缺点 计算复杂度高，因为对于每一个新的数据点，都需要与所有训练数据计算距离。 需要决定 K 的大小，这可能会影响结果。 高维数据中的性能下降。\n应用场景： 推荐系统:\n基于用户之前的喜好推荐相似电影 推荐用户可能喜欢的曲目或歌手 文本分类: 区分垃圾邮件和正常邮件。\n图像识别: 识别包括上的手写邮政编码，分类投递邮件包裹\n医疗诊断： 预测患者可能的疾病风险。\n信用评分：预测客户的信用风险。\n欺诈检测：识别信用卡中的异常交易。\n位置基服务：基于位置提供餐厅或服务推荐。\n","permalink":"https://www.devean.cn/post/2023-11-12-machine-learning-k-nearest-neighbours/","title":"K临近(KNN) | 机器学习"},{"contents":" 本文从机器学习与传统建模区别、机器学习分类：监督、非监督、半监督、强化，基础算法：K临近(KNN)、K均值(KMC)、朴素贝叶斯(NBC)、支持向量机(SVM)、回归、自组织映射、神经网络原理(NLP)\n什么是机器学习 机器学习是人工智能的一个分支,它让计算机从数据中自动“学”到知识,并用这些知识做决策或预测,而不需要我们一步步明确地告诉它怎么做。\n传统数学 vs 机器学习 数学建模 机器学习 相同点 数据驱动: 两者都利用数据来构建和验证模型。\n预测和推断: 数学建模和机器学习都可以用于预测未知的输出或解释数据中的模式。\n优化问题: 在某些情况下，两者都可能涉及到优化问题，例如，寻找最小化误差的参数。\n不同点 目的 数学建模：旨在用数学的形式来描述现实世界中的现象或问题，往往为了理解其背后的机制或原理。\n主要关注的是预测和泛化。机器学习模型可能不太关心背后的机制，而是关心在未知数据上的性能。\n模型构建 数学建模：模型的形式通常基于对现象的物理、生物或经济学的理解。例如，描述人口增长的模型可能基于出生率和死亡率的估计。\n机器学习：模型的形式主要基于数据。使用的模型可能没有明确的现实意义，例如深度学习模型。\n验证 数学建模：模型的验证通常基于其是否与现实世界的观察相符合，以及其是否可以提供洞察力。\n机器学习：验证通常基于模型在独立测试集上的性能。\n模型的解释性 数学建模：模型往往更具解释性，因为它们是基于现象的某些已知原理或规律构建的。\n机器学习：尤其是某些复杂的模型，如深度神经网络，可能难以解释。尽管如此，机器学习领域也有许多工作在努力提高模型的可解释性。\n应用 数学建模：常应用于工程、物理学、经济学等领域，以帮助专家了解和控制系统。\n机器学习：广泛应用于计算机视觉、自然语言处理、推荐系统等领域，主要关注自动化和预测。\n模型复杂性 数学建模：往往倾向于使用更简单的、基于物理学或其他学科原理的模型。\n机器学习：可能使用非常复杂的模型，特别是当数据量大且复杂度高时。\n总的来说，数学建模和机器学习都是理解、解释和预测现象的工具，但它们的关注点、方法和应用有所不同。\n主要类别 机器学习主要分为三大类：监督学习、非监督学习、强化学习，而监督学习和非监督学习中又衍生出半监督学习。\n监督学习 Supervised Learning是机器学习的一种方法，其模型是通过输入-输出(有标签的数据)对进行训练，目标是从给定的数据中学习一个映射函数,以便在给定新的输入时，模型可以预测相应的输出 。训练过程涉及到输入数据和其相应的标签，并尝试找到这两者之间的关系。一旦模型被训练，它可以用来预测新、未标签数据的输出。\n非监督学习 Unsupervised Learning模型被训练在没有标签的数据上。它的目的是学习数据的底层结构、分布或表示，而不是预测标签。与监督学习不同，非监督学习的目标并不是预测一个输出。相反，它试图通过某种方式学习数据的结构，这可以是通过聚类、降维或生成模型等方式来实现的。\n强化学习 Reinforcement Learning是通过与环境交互来学习如何行动，从而最大化某种定义的长期回报。与传统的监督学习不同，强化学习通常涉及决策问题，其中每个行动都会影响未来的回报。\n半监督学习 Semi-supervised Learning是介于两个极端之间(监督式是指整个数据集被标记，而非监督式是指没有标记)。半监督学习任务具有一个标记和一个未标记的数据集。它使用未标记的数据来获得对数据结构的更多理解。通常，SSL使用小的带标签数据集和较大的未带标签数据集来进行学习。)学习正如其名称所示，介于两个极端之间(监督式是指整个数据集被标记，而非监督式是指没有标记)。半监督学习任务具有一个标记和一个未标记的数据集。它使用未标记的数据来获得对数据结构的更多理解。通常，SSL使用小的带标签数据集和较大的未带标签数据集来进行学习。\n为什么会用半监督学习 半监督学习位于监督学习和非监督学习之间，利用少量的标记数据和大量的未标记数据进行学习。以下是为什么要使用半监督学习的原因：\n数据标注成本高 ：在很多应用中，收集大量数据是相对容易的，但为这些数据打标签则既昂贵又耗时。例如，在医学图像领域，一个专家可能需要花费大量时间来手动标注图像中的特定结构或病变。利用半监督学习，可以用少量的标注数据和大量的未标注数据共同训练模型。\n利用数据的完整潜力 ：未标记的数据包含有关数据分布的有用信息。半监督学习方法尝试利用这些信息来改善模型的性能。\n提高泛化能力 ：在某些情况下，利用大量的未标记数据可以帮助模型更好地泛化到新的、未见过的数据。\n数据标注可能存在误差 ：在某些场景中，即使数据被标注，标签也可能存在噪音或误差。在这种情况下，使用半监督学习方法，结合大量的未标记数据，可能会提供一个更稳健的学习策略。\n在某些任务中，有很多相关的未标记数据 ：例如，在自然语言处理中，我们可能有少量标记的数据集，但可以从网络上轻松获得大量的相关文本。半监督学习可以利用这些未标记的文本来提高模型的性能。\n适应数据的变化 ：在动态环境中，数据分布可能随时间而变化。利用半监督学习，可以定期利用新收集的未标记数据来更新模型，使其适应变化。\n总之，半监督学习提供了一种在有限标记数据的情况下利用未标记数据的方法，这对于许多实际应用来说是非常有价值的。\n模型 K近临 K Nearest-Neighbours是一种监督学习技术，给定一个新的观测值，KNN算法会从训练数据集中搜索出k个与其最相似的实例，然后基于这些邻居的属性来预测新观测值的标签。\n原理 距离计算 ：对于给定的新数据点，计算它与训练数据集中每个点的距离。\n选取K个邻居 ：从训练数据集中选取距离最近的K个点。\n投票 (对于分类) ：对于K个邻居，看哪个类别最为常见，并将其指定为新数据点的类别。\n平均 (对于回归) ：对于K个邻居，计算其属性的平均值，并将其指定为新数据点的值。\nK均值 K-Means Clustering是一种无监督的聚类算法，其目的是将n个数据点分为k个聚类。每个聚类都有一个中心，这些中心最小化了其内部数据点与中心之间的距离。\n原理 初始化： 随机选择k个数据点作为初始聚类中心。\n分配： 为每个数据点分配最近的聚类中心。\n更新： 计算每个聚类的平均值，并将平均值设为新的聚类中心。\n重复： 重复步骤2和3，直到聚类中心不再显著变化。\n朴素贝叶斯 Naive Bayes Classifier监督学习技术是基于贝叶斯定理的一种简单概率分类器。它假设特征之间是独立的（这就是“朴素”一词的来源），即一个特征的出现不会影响另一个特征的出现。\n原理 给定一个类别 C 和一个特定的特 x ，贝叶斯定理表示为：\n其中\nP(C|x) 是在给定特征 x 的情况下类别 C 的后验概率。\nP(x|C) 是在类别 C 的情况下观察到特征 x 的概率。\nP(C) 是类别 C 的先验概率。\nP(x) 是观察到特征 x 的概率。\n对于分类问题，我们可以忽略分母（因为它对所有的类别都是相同的）并计算每个类别的$P(C)×P(x∣C)$。我们将数据点分类为给出最大值的类别。\n回归方法 Regression Methods是一种预测性监督学习技术，它研究的是因变量(目标)和自变量(预测器)之间的关系。 这种技术通常用于预测分析，时间序列模型以及发现变量之间的因果关系。\n支持向量机 Support Vector Machines, SVM）是一种在分类和回归分析中使用的监督学习模型，基本思想是找到一个超平面，使得两个类别之间的边界最大化。对于线性可分的数据，这意味着超平面可以完美地分隔两个类别，并且最大化与最近的训练数据点（即支持向量）之间的距离。SVM使用所谓的核方法进行扩展。核方法的思想是将数据映射到一个更高维度的空间，使其在新的空间中变得线性可分。\n自组织映射 Self-Organizing Maps，简称SOM是一种无监督学习的神经网络，SOM是一种将高维数据映射到通常是二维（有时是三维）的网格结构上的方法。与其他神经网络不同，SOM没有激活函数，它根据输入特征的相似性将相似的输入向量组合在一起。\n原理 初始化 ：为每个节点（或称为神经元）随机初始化一个权重向量。\n竞争 ：对于每个输入样本，找到与其最相似（即欧几里得距离最近）的权重向量的节点，这个节点被称为胜者节点（winning node）。\n适应 ：更新胜者节点和其邻居节点的权重向量，使它们更接近当前输入样本。邻居节点的定义和更新的幅度都随时间逐渐减少。\n迭代 ：对于大量的迭代，重复上述步骤，直到模型收敛。\n决策树 Decision Trees是一种监督学习模型、主要用于分类和回归任务，决策树是一个树形结构，其中每个内部节点表示一个特征属性上的测试，每个分支代表一个测试结果，每个叶节点代表一个类别（在分类任务中）或连续的值（在回归任务中）。\n原理 决策树通过一系列基于特征值的测试，将输入数据点分配到叶节点中的一个，从而完成分类或预测任务。\n神经网络 Neural Networks是一种模仿生物神经网络结构和功能的计算模型。\n基本结构 神经元（Neuron） : 神经网络的基本单元。每个神经元接收一个或多个输入，加权处理这些输入，然后产生一个输出。\n层（Layer） ：神经网络由多层神经元组成。主要有三种类型的层：\n输入层（Input Layer） ：接收外部数据的层。\n隐藏层（Hidden Layers） ：在输入层和输出层之间的层，可以有多个。\n输出层（Output Layer） ：产生最终预测或分类的层。\n工作原理 每个神经元的输入都与一个权重相乘，所有加权输入的总和加上一个偏置，然后传递给激活函数。激活函数的输出是该神经元的输出。\n欢迎扫码关注公众号，订阅更多文章!\n","permalink":"https://www.devean.cn/post/2023-09-08-machine-learning-basic-concepts/","title":"机器学习 | 基础概念"},{"contents":"环新疆摩旅 版权声明：以下图片版权归本人所有，禁止一切商用，如需商用请与本人取得联系获取授权，侵权必究！ 2023.8.2 北京\u0026ndash;巴彦淖尔临河服务区 2023.8.3 巴彦淖尔临河服务区\u0026ndash;额济纳旗 2023.8.3 额济纳旗\u0026ndash;哈密 2023.8.5 哈密-巴里坤 阿勒吞古街 哈密文化馆 哈密回王墓 天山庙 巴里坤草原 2023.8.6 巴里坤-吐鲁番-乌鲁木齐 巴里坤糊 天山无人区峡谷 吐峪沟麻扎村 2023.8.7 乌鲁木齐 2023.8.8 阿勒泰五彩湾 2023.8.9 可可托海 2023.8.10 禾木 2023.8.11-12 喀纳斯 2023.8.13 吉木乃 草原石城 吉木乃草原 松海湾 2023.8.16 塞里湖\u0026ndash;尼勒克 赛里木湖\u0026ndash;大西洋的最后一滴泪 果子沟大桥 2023.8.17 尼勒克\u0026ndash;奎屯 特克斯八卦城 2023.8.19 奎屯\u0026ndash;独库640兰萨德克服务区 安集海大峡谷 2023.8.20 独库640兰萨德克服务区\u0026ndash;天山神秘大峡谷 ","permalink":"https://www.devean.cn/post/2023-08-02-motorcycle-tour-around-xinjiang/","title":"环新疆摩旅"},{"contents":"Nginx 下载安装 Web服务部署 Nginx 详细配置 年费CA证书配置 安装certbot yum install certbot 手动只安装证书 certbot run -a manual -i nginx -d domain.com,www.domain.com Saving debug log to /var/log/letsencrypt/letsencrypt.log Requesting a certificate for www.domain.com - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - Create a file containing just this data: I7Sl7-xakxMT9dCS67OrHmzn_tFiBx-64g58jlkj9FM.LdWLRuDDIqZUWnece6JOlugrqigifvupPi5EXSfWi0M And make it available on your web server at this URL: http://www.yuntun.com/.well-known/acme-challenge/I7Sl7-xakxMT9dCS67OrHmzn_tFiBx-64g58jlkj9FM - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - Press Enter to Continue 创建验证文件 按上面提示创建对应的文件 自己验证http文件可访问 继续按continue 重启nginx服务 ","permalink":"https://www.devean.cn/post/2023-12-11-nginx%E9%83%A8%E7%BD%B2web%E6%9C%8D%E5%8A%A1%E5%8F%8A%E5%9F%9F%E5%90%8D%E5%85%8D%E8%B4%B9ca%E8%AE%A4%E8%AF%81%E8%AF%A6%E7%BB%86%E6%B5%81%E7%A8%8B/","title":"Nginx部署web服务及域名免费CA认证详细流程"},{"contents":"About Me Devean Ye is\n","permalink":"https://www.devean.cn/about/","title":""},{"contents":"Java 语言学习笔记 ","permalink":"https://www.devean.cn/notes/","title":""},{"contents":"","permalink":"https://www.devean.cn/archive/","title":"Posts Archive"},{"contents":"","permalink":"https://www.devean.cn/search/search/","title":"搜索"}]