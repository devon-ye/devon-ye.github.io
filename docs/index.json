[{"author":null,"categories":["Tech"],"content":"非线性支持向量机的核技巧、核函数、决策函数，股价预测模型、总结支持向量机目标函数","date":1701475200,"description":"非线性支持向量机的核技巧、核函数、决策函数，股价预测模型、总结支持向量机目标函数","dir":"post/","excerpt_html":"非线性支持向量机的核技巧、核函数、决策函数，股价预测模型、总结支持向量机目标函数","excerpt_text":"非线性支持向量机的核技巧、核函数、决策函数，股价预测模型、总结支持向量机目标函数","expirydate":-62135596800,"fuzzywordcount":2400,"html":"非线性支持向量机的核技巧、核函数、决策函数，股价预测模型、总结支持向量机目标函数","keywords":null,"kind":"page","lang":"en","lastmod":1701475200,"objectID":"77cf87d983ce0ac86a3bf4a52ff23a5a","permalink":"https://www.devean.cn/post/2023-12-02-machine-learning-nonlinear-support-vector-machines/","publishdate":"2023-12-02T00:00:00Z","readingtime":5,"relpermalink":"/post/2023-12-02-machine-learning-nonlinear-support-vector-machines/","section":"post","summary":"非线性支持向量机（SVM）是一种强大的监督学习算法，用于解决分类和回归问题。它通过使用核技巧将数据映射到高维空间，从而处理非线性关系。在这篇","tags":["Machine Learning"],"title":"机器学习 | 非线性支持向量机","type":"post","url":"/post/2023-12-02-machine-learning-nonlinear-support-vector-machines/","weight":0,"wordcount":2311},{"author":null,"categories":["Tech"],"content":"支持向量机线性不可分、软间隔、松弛变量、目标函数、约束条件、超参数 C,实际应用场景判别线性是否可分等几方面讲概述了支持向量机线性不可分","date":1700352000,"description":"支持向量机线性不可分、软间隔、松弛变量、目标函数、约束条件、超参数 C,实际应用场景判别线性是否可分等几方面讲概述了支持向量机线性不可分","dir":"post/","excerpt_html":"支持向量机线性不可分、软间隔、松弛变量、目标函数、约束条件、超参数 C,实际应用场景判别线性是否可分等几方面讲概述了支持向量机线性不可分","excerpt_text":"支持向量机线性不可分、软间隔、松弛变量、目标函数、约束条件、超参数 C,实际应用场景判别线性是否可分等几方面讲概述了支持向量机线性不可分","expirydate":-62135596800,"fuzzywordcount":1900,"html":"支持向量机线性不可分、软间隔、松弛变量、目标函数、约束条件、超参数 C,实际应用场景判别线性是否可分等几方面讲概述了支持向量机线性不可分","keywords":null,"kind":"page","lang":"en","lastmod":1700352000,"objectID":"c49cd9a9e6adbb540a0116a0105ef7a5","permalink":"https://www.devean.cn/post/2023-11-19-machine-learning-support-vector-machine-non-linearly-separable/","publishdate":"2023-11-19T00:00:00Z","readingtime":4,"relpermalink":"/post/2023-11-19-machine-learning-support-vector-machine-non-linearly-separable/","section":"post","summary":"本文从支持向量机线性不可分、软间隔、松弛变量、目标函数、约束条件、超参数 C,实际应用场景判别线性是否可分等几方面讲概述了支持向量机线性不可分","tags":["Machine Learning"],"title":"机器学习 | 支持向量机线性不可分","type":"post","url":"/post/2023-11-19-machine-learning-support-vector-machine-non-linearly-separable/","weight":0,"wordcount":1819},{"author":null,"categories":["Tech"],"content":"支持向量机概念、硬间隔、软间隔和非线性的区别、原理、术语、最大间隔数学推导几个方面详细讲解线性可分的支持向量机","date":1700265600,"description":"支持向量机概念、硬间隔、软间隔和非线性的区别、原理、术语、最大间隔数学推导几个方面详细讲解线性可分的支持向量机","dir":"post/","excerpt_html":"支持向量机概念、硬间隔、软间隔和非线性的区别、原理、术语、最大间隔数学推导几个方面详细讲解线性可分的支持向量机","excerpt_text":"支持向量机概念、硬间隔、软间隔和非线性的区别、原理、术语、最大间隔数学推导几个方面详细讲解线性可分的支持向量机","expirydate":-62135596800,"fuzzywordcount":1900,"html":"支持向量机概念、硬间隔、软间隔和非线性的区别、原理、术语、最大间隔数学推导几个方面详细讲解线性可分的支持向量机","keywords":null,"kind":"page","lang":"en","lastmod":1700265600,"objectID":"fc110a56be710a12dfd5c35c78fb3545","permalink":"https://www.devean.cn/post/2023-11-18-machine-learning-support-vector-machine-linearly-separable/","publishdate":"2023-11-18T00:00:00Z","readingtime":4,"relpermalink":"/post/2023-11-18-machine-learning-support-vector-machine-linearly-separable/","section":"post","summary":"本文从支持向量机概念、硬间隔、软间隔和非线性的区别、原理、术语、最大间隔数学推导几个方面详细讲解线性可分的支持向量机。 基础概念 支持向量机(S","tags":["Machine Learning"],"title":"机器学习 | 支持向量机线性可分","type":"post","url":"/post/2023-11-18-machine-learning-support-vector-machine-linearly-separable/","weight":0,"wordcount":1802},{"author":null,"categories":["Finance"],"content":"简单科普了什么是套利、有哪些套利方法、套利失效的原因。","date":1700060371,"description":"简单科普了什么是套利、有哪些套利方法、套利失效的原因。","dir":"post/","excerpt_html":"简单科普了什么是套利、有哪些套利方法、套利失效的原因。","excerpt_text":"简单科普了什么是套利、有哪些套利方法、套利失效的原因。","expirydate":-62135596800,"fuzzywordcount":900,"html":"简单科普了什么是套利、有哪些套利方法、套利失效的原因。","keywords":["套利","Arbitrage","量化金融","Quantitative Finance"],"kind":"page","lang":"en","lastmod":1700060371,"objectID":"702a6aab9686b421c6068d90f9aa620a","permalink":"https://www.devean.cn/post/2023-11-15-what-is-arbitrage/","publishdate":"2023-11-15T22:59:31+08:00","readingtime":2,"relpermalink":"/post/2023-11-15-what-is-arbitrage/","section":"post","summary":"本文简单科普了什么是套利、有哪些套利方法、套利失效的原因。 什么是套利 套利是指在无风险回报率之外获得确定的利润。 用量化金融的语言来说，我们 可以","tags":["Quantitative Finance"],"title":"什么是套利 | 翻译","type":"post","url":"/post/2023-11-15-what-is-arbitrage/","weight":0,"wordcount":835},{"author":null,"categories":["Finance"],"content":"量化金融中数学的应用、常见的建模方法、常见的数学工具普及了量化金融中的应用","date":1699973971,"description":"量化金融中数学的应用、常见的建模方法、常见的数学工具普及了量化金融中的应用","dir":"post/","excerpt_html":"量化金融中数学的应用、常见的建模方法、常见的数学工具普及了量化金融中的应用","excerpt_text":"量化金融中数学的应用、常见的建模方法、常见的数学工具普及了量化金融中的应用","expirydate":-62135596800,"fuzzywordcount":400,"html":"量化金融中数学的应用、常见的建模方法、常见的数学工具普及了量化金融中的应用","keywords":null,"kind":"page","lang":"en","lastmod":1699973971,"objectID":"6a3aebed5e26a09f8b3f17ba5f95c8b8","permalink":"https://www.devean.cn/post/2023-11-14-mathematical-applications-in-quantitative-finance/","publishdate":"2023-11-14T22:59:31+08:00","readingtime":1,"relpermalink":"/post/2023-11-14-mathematical-applications-in-quantitative-finance/","section":"post","summary":"本文量从量化金融中数学的应用、常见的建模方法、常见的数学工具普及了量化金融中的应用。 量化金融中有哪些不同类型的数学？ 在数量金融中使用最多的数","tags":["Quantitative Finance"],"title":"量化金融中的数学应用 | 翻译","type":"post","url":"/post/2023-11-14-mathematical-applications-in-quantitative-finance/","weight":0,"wordcount":379},{"author":null,"categories":["Tech"],"content":"特征缩放(Feature Scaling)概念、目的、常用特征缩放方法：最小-最大缩放、标准缩放、鲁棒缩放、L2 Normalization、L1 Normalization、Power Transformer的公式讲解、Python缩放数据可视化对比诠释了特征缩放","date":1699660800,"description":"特征缩放(Feature Scaling)概念、目的、常用特征缩放方法：最小-最大缩放、标准缩放、鲁棒缩放、L2 Normalization、L1 Normalization、Power Transformer的公式讲解、Python缩放数据可视化对比诠释了特征缩放","dir":"post/","excerpt_html":"特征缩放(Feature Scaling)概念、目的、常用特征缩放方法：最小-最大缩放、标准缩放、鲁棒缩放、L2 Normalization、L1 Normalization、Power Transformer的公式讲解、Python缩放数据可视化对比诠释了特征缩放","excerpt_text":"特征缩放(Feature Scaling)概念、目的、常用特征缩放方法：最小-最大缩放、标准缩放、鲁棒缩放、L2 Normalization、L1 Normalization、Power Transformer的公式讲解、Python缩放数据可视化对比诠释了特征缩放","expirydate":-62135596800,"fuzzywordcount":1900,"html":"特征缩放(Feature Scaling)概念、目的、常用特征缩放方法：最小-最大缩放、标准缩放、鲁棒缩放、L2 Normalization、L1 Normalization、Power Transformer的公式讲解、Python缩放数据可视化对比诠释了特征缩放","keywords":null,"kind":"page","lang":"en","lastmod":1699660800,"objectID":"a3d1bc0297d485650c412b0fe2cfd17a","permalink":"https://www.devean.cn/post/2023-11-11-machine-learning-feature-scaling/","publishdate":"2023-11-11T00:00:00Z","readingtime":4,"relpermalink":"/post/2023-11-11-machine-learning-feature-scaling/","section":"post","summary":"\u003cblockquote\u003e\n\u003cp\u003e本文从特征缩放概念、目的、常用特征缩放方法：最小-最大缩放、标准缩放、鲁棒缩放、L2 Normalization、L1 Normalization、Power Transformer的公式讲解、Python缩放数据可视化对比诠释了特征缩放\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2 id=\"什么是特征缩放\"\u003e什么是特征缩放\u003c/h2\u003e\n\u003cp\u003e特征缩放又称归一化，是机器学习中的一种技术，涉及调整数值数据的量度，使所有数据点在相似的尺度上。例如：身高、体重、年龄、收入等个人特征数据，每个维度的区间不一样，为保证所有维度的特征数据尺度一样，我们就需要对原始数据做特征缩放，将身高、体重、年龄、收入都转化为区间[0,1]之间的数据。\u003c/p\u003e","tags":["Machine Learning"],"title":"机器学习 | 特征缩放","type":"post","url":"/post/2023-11-11-machine-learning-feature-scaling/","weight":0,"wordcount":1829},{"author":null,"categories":["Tech"],"content":"K临近(KNN)的概念、原理、距离函数、K 值选择、K 值影响、、优缺点、应用几方面详细讲述了 KNN 算法","date":1694304000,"description":"K临近(KNN)的概念、原理、距离函数、K 值选择、K 值影响、、优缺点、应用几方面详细讲述了 KNN 算法","dir":"post/","excerpt_html":"K临近(KNN)的概念、原理、距离函数、K 值选择、K 值影响、、优缺点、应用几方面详细讲述了 KNN 算法","excerpt_text":"K临近(KNN)的概念、原理、距离函数、K 值选择、K 值影响、、优缺点、应用几方面详细讲述了 KNN 算法","expirydate":-62135596800,"fuzzywordcount":2100,"html":"K临近(KNN)的概念、原理、距离函数、K 值选择、K 值影响、、优缺点、应用几方面详细讲述了 KNN 算法","keywords":null,"kind":"page","lang":"en","lastmod":1694304000,"objectID":"80b02e6cd32e266756e5dd3450c43a77","permalink":"https://www.devean.cn/post/2023-11-12-machine-learning-k-nearest-neighbours/","publishdate":"2023-09-10T00:00:00Z","readingtime":5,"relpermalink":"/post/2023-11-12-machine-learning-k-nearest-neighbours/","section":"post","summary":"本文从概念、原理、距离函数、K 值选择、K 值影响、、优缺点、应用几方面详细讲述了 KNN 算法。 K 近临(K Nearest-Neighbours) 一种简单的监督学习算法，惰性学习算法，在技","tags":["Machine Learning"],"title":"K临近(KNN) | 机器学习","type":"post","url":"/post/2023-11-12-machine-learning-k-nearest-neighbours/","weight":0,"wordcount":2091},{"author":null,"categories":["Tech"],"content":"机器学习与传统建模区别、机器学习分类：监督、非监督、半监督、强化，基础算法：K临近、K均值、朴素贝叶斯、支持向量机、回归、自组织映射、神经网络原理","date":1694131200,"description":"机器学习与传统建模区别、机器学习分类：监督、非监督、半监督、强化，基础算法：K临近、K均值、朴素贝叶斯、支持向量机、回归、自组织映射、神经网络原理","dir":"post/","excerpt_html":"机器学习与传统建模区别、机器学习分类：监督、非监督、半监督、强化，基础算法：K临近、K均值、朴素贝叶斯、支持向量机、回归、自组织映射、神经网络原理","excerpt_text":"机器学习与传统建模区别、机器学习分类：监督、非监督、半监督、强化，基础算法：K临近、K均值、朴素贝叶斯、支持向量机、回归、自组织映射、神经网络原理","expirydate":-62135596800,"fuzzywordcount":3800,"html":"机器学习与传统建模区别、机器学习分类：监督、非监督、半监督、强化，基础算法：K临近、K均值、朴素贝叶斯、支持向量机、回归、自组织映射、神经网络原理","keywords":null,"kind":"page","lang":"en","lastmod":1694131200,"objectID":"ee8bf0d96d30c4b78f4c18fe77e77bb8","permalink":"https://www.devean.cn/post/2023-09-08-machine-learning-basic-concepts/","publishdate":"2023-09-08T00:00:00Z","readingtime":8,"relpermalink":"/post/2023-09-08-machine-learning-basic-concepts/","section":"post","summary":"本文从机器学习与传统建模区别、机器学习分类：监督、非监督、半监督、强化，基础算法：K临近(KNN)、K均值(KMC)、朴素贝叶斯(NBC)、","tags":["Machine Learning"],"title":"机器学习 | 基础概念","type":"post","url":"/post/2023-09-08-machine-learning-basic-concepts/","weight":0,"wordcount":3770},{"author":null,"categories":["Life"],"content":"","date":1690934400,"description":"","dir":"post/","excerpt_html":"","excerpt_text":"","expirydate":-62135596800,"fuzzywordcount":200,"html":"","keywords":null,"kind":"page","lang":"en","lastmod":1690934400,"objectID":"32690ed761b13b3d7023afc15b8ca4d4","permalink":"https://www.devean.cn/post/2023-08-02-motorcycle-tour-around-xinjiang/","publishdate":"2023-08-02T00:00:00Z","readingtime":1,"relpermalink":"/post/2023-08-02-motorcycle-tour-around-xinjiang/","section":"post","summary":"环新疆摩旅 版权声明：以下图片版权归本人所有，禁止一切商用，如需商用请于本人取得联系获取授权，侵权必究！ 2023.8.2 北京\u0026ndash;巴彦淖尔临河服务区","tags":["Travel"],"title":"环新疆摩旅","type":"post","url":"/post/2023-08-02-motorcycle-tour-around-xinjiang/","weight":0,"wordcount":167},{"author":null,"categories":["Tech"],"content":"Nginx下载安装，完整配置、Web部署、免费CA证书配置全流程","date":1528070400,"description":"Nginx下载安装，完整配置、Web部署、免费CA证书配置全流程","dir":"post/","excerpt_html":"Nginx下载安装，完整配置、Web部署、免费CA证书配置全流程","excerpt_text":"Nginx下载安装，完整配置、Web部署、免费CA证书配置全流程","expirydate":-62135596800,"fuzzywordcount":300,"html":"Nginx下载安装，完整配置、Web部署、免费CA证书配置全流程","keywords":null,"kind":"page","lang":"en","lastmod":1528070400,"objectID":"a2da16a19a485b03f87c3c56bcdbe478","permalink":"https://www.devean.cn/post/2023-12-11-nginx%E9%83%A8%E7%BD%B2web%E6%9C%8D%E5%8A%A1%E5%8F%8A%E5%9F%9F%E5%90%8D%E5%85%8D%E8%B4%B9ca%E8%AE%A4%E8%AF%81%E8%AF%A6%E7%BB%86%E6%B5%81%E7%A8%8B/","publishdate":"2018-06-04T00:00:00Z","readingtime":1,"relpermalink":"/post/2023-12-11-nginx%E9%83%A8%E7%BD%B2web%E6%9C%8D%E5%8A%A1%E5%8F%8A%E5%9F%9F%E5%90%8D%E5%85%8D%E8%B4%B9ca%E8%AE%A4%E8%AF%81%E8%AF%A6%E7%BB%86%E6%B5%81%E7%A8%8B/","section":"post","summary":"Nginx 下载安装 Web服务部署 Nginx 详细配置 年费CA证书配置 安装certbot yum install certbot 手动只安装证书 certbot run -a manual -i nginx -d domain.com,www.domain.com Saving debug log to /var/log/letsencrypt/letsencrypt.log Requesting a certificate for www.domain.com - - - - - - - -","tags":["tag1","tag2"],"title":"Nginx部署web服务及域名免费CA认证详细流程","type":"post","url":"/post/2023-12-11-nginx%E9%83%A8%E7%BD%B2web%E6%9C%8D%E5%8A%A1%E5%8F%8A%E5%9F%9F%E5%90%8D%E5%85%8D%E8%B4%B9ca%E8%AE%A4%E8%AF%81%E8%AF%A6%E7%BB%86%E6%B5%81%E7%A8%8B/","weight":0,"wordcount":217},{"author":null,"categories":null,"content":null,"date":-62135596800,"description":"","dir":"about/","excerpt_html":null,"excerpt_text":null,"expirydate":-62135596800,"fuzzywordcount":100,"html":null,"keywords":null,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"8576ec274c98b3831668a172fa632d80","permalink":"https://www.devean.cn/about/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/about/","section":"","summary":"About Me Devean Ye is","tags":null,"title":"","type":"page","url":"/about/","weight":0,"wordcount":5},{"author":null,"categories":null,"content":null,"date":-62135596800,"description":"","dir":"notes/","excerpt_html":null,"excerpt_text":null,"expirydate":-62135596800,"fuzzywordcount":100,"html":null,"keywords":null,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"1ede8046f9c3a02d422dea7bbf324e64","permalink":"https://www.devean.cn/notes/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/notes/","section":"","summary":"Java 语言学习笔记","tags":null,"title":"","type":"page","url":"/notes/","weight":0,"wordcount":7},{"author":null,"categories":null,"content":null,"date":-62135596800,"description":"","dir":"search/","excerpt_html":null,"excerpt_text":null,"expirydate":-62135596800,"fuzzywordcount":100,"html":null,"keywords":null,"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"e70fff4f74e107cdcde5c9e2abcfc149","permalink":"https://www.devean.cn/search/","publishdate":"0001-01-01T00:00:00Z","readingtime":0,"relpermalink":"/search/","section":"search","summary":"","tags":null,"title":"","type":"search","url":"/search/","weight":0,"wordcount":0},{"author":null,"categories":null,"content":"Archive of historical posts.","date":-62135596800,"description":"Archive of historical posts.","dir":"archive/","excerpt_html":"Archive of historical posts.","excerpt_text":"Archive of historical posts.","expirydate":-62135596800,"fuzzywordcount":100,"html":"Archive of historical posts.","keywords":null,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"a06e5ce9eca4c3260843078104889780","permalink":"https://www.devean.cn/archive/","publishdate":"0001-01-01T00:00:00Z","readingtime":0,"relpermalink":"/archive/","section":"","summary":"","tags":null,"title":"Posts Archive","type":"archive","url":"/archive/","weight":0,"wordcount":0},{"author":null,"categories":null,"content":null,"date":-62135596800,"description":"","dir":"search/","excerpt_html":null,"excerpt_text":null,"expirydate":-62135596800,"fuzzywordcount":100,"html":null,"keywords":null,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"8946788897930c0c0c39fbfcd30ff2e4","permalink":"https://www.devean.cn/search/search/","publishdate":"0001-01-01T00:00:00Z","readingtime":0,"relpermalink":"/search/search/","section":"search","summary":"","tags":null,"title":"搜索","type":"search","url":"/search/search/","weight":0,"wordcount":0},{"contents":" 非线性支持向量机（SVM）是一种强大的监督学习算法，用于解决分类和回归问题。它通过使用核技巧将数据映射到高维空间，从而处理非线性关系。在这篇文章中，我们将探讨非线性 SVM 的工作原理、核函数的作用以及如何在实际中应用非线性 SVM。\n核心概念 线性与非线性 SVM 线性 SVM 在处理线性可分数据时效果显著。然而，当数据集呈非线性分布时，线性 SVM 的性能会受限。非线性 SVM 通过核技巧解决了这个问题。\n核技巧（Kernel Trick） 什么是核技巧？ 核技巧是一种通过转换将低维输入空间映射到高维特征空间的方法，它使得非线性特征组合可以被 SVM 以线性方式处理，核技巧由一些数学工具核函数实现。\n核函数定义 核函数是一个数学函数，能够计算数据点在高维特征空间中的内积，而无需直接计算这些特征。换句话说，它可以让我们在原始特征空间中间接地计算在更高维特征空间的内积。\n核函数的作用 维度映射： 核函数通过隐式地将数据映射到一个更高维的空间，帮助处理数据的非线性特征。 计算简化： 直接在高维空间中处理数据是复杂和计算密集的。核函数通过在原始空间中进行计算来避免这个问题，从而简化了计算过程。 处理非线性问题： 在许多实际问题中，数据集无法用线性方法分割。核函数使 SVM 能够通过在高维空间中寻找线性边界来处理这些非线性问题。 常见的核函数 高斯径向基函数（RBF）核： 最常用的核函数，适合处理没有明显特征模式的复杂数据集。 $$K(\\mathbf{x}_i, \\mathbf{x}_j) = \\exp(-\\gamma ||\\mathbf{x}_i - \\mathbf{x}_j||^2)$$ 其中，$\\gamma$ 是缩放参数。\n多项式核： 用于将数据映射到由原始特征的多项式组成的高维空间。 $$K(\\mathbf{x}_i, \\mathbf{x}_j) = (\\gamma \\mathbf{x}_i \\cdot \\mathbf{x}_j + r)^d$$\n其中，$\\gamma$ 是缩放参数，$d$ 是多项式的度数,$ r $ 是系数项。\nSigmoid 核： 类似于神经网络的激活函数。\n$$K(\\mathbf{x}_i, \\mathbf{x}_j) = anh(\\gamma \\mathbf{x}_i \\cdot \\mathbf{x}_j + r)$$ 其中，$\\gamma$ 是缩放参数，$r$ 是偏置项。\n目标函数引入核函数 对于非线性数据集，目标函数和约束条件与线性不可分 SVM 类似，但使用核技巧在高维空间中寻找最优超平面。\n目标函数:\n$\\max_{\\alpha} \\sum_{i=1}^n \\alpha_i - \\frac{1}{2} \\sum_{i,j=1}^n y_i y_j \\alpha_i \\alpha_j K(\\mathbf{x}_i, \\mathbf{x}_j)$\n约束条件调整:\n$\\sum_{i=1}^n y_i \\alpha_i = 0 \\text{ and } 0 \\leq \\alpha_i \\leq C, \\quad \\forall i$\n构建决策函数 在找到最优超平面后，SVM 使用决策函数来评估新样本属于哪个类别。 $$ f(x)=sign(∑_{i=1}^n​y_i​α_i​K(x_i​,x)+b) $$\n应用实例 支持向量机用于预测股票价格（例如 标普 500ETF，代码为 \u0026lsquo;SPY\u0026rsquo;）的未来 \u0026rsquo;n\u0026rsquo; 天收盘价的机器学习模型。\n# 基础库 import pandas as pd import numpy as np import matplotlib.pyplot as plt from matplotlib.font_manager import FontProperties plt.style.use(\u0026#39;fivethirtyeight\u0026#39;) # 设置中文字体 fontPath = \u0026#39;/System/Library/Fonts/PingFang.ttc\u0026#39; myFont = FontProperties(fname=fontPath) # 预处理 from sklearn.preprocessing import MinMaxScaler from sklearn.pipeline import Pipeline from sklearn.model_selection import train_test_split, TimeSeriesSplit, GridSearchCV # SVM from sklearn.svm import SVR # 忽略警告 import warnings warnings.filterwarnings(\u0026#39;ignore\u0026#39;) # 使用 yfinance 加载数据 import yfinance as yf # 创建变量来预测 \u0026#39;n\u0026#39; 天之后的价格 n = 5 # 从 yfinance 加载 SPY 的数据 df = yf.download(\u0026#39;SPY\u0026#39;, start=\u0026#39;2020-01-01\u0026#39;, end=\u0026#39;2023-12-02\u0026#39;, progress=False) df = df[[\u0026#39;Adj Close\u0026#39;]] # 打印数据的最后 5 行 print(df.tail()) # 创建目标变量 df[\u0026#39;Target\u0026#39;] = df[\u0026#39;Adj Close\u0026#39;].shift(-n) print(df.tail(6)) # 构建预测变量和目标变量 X = df[[\u0026#39;Adj Close\u0026#39;]].values[:-n] y = df[\u0026#39;Target\u0026#39;].values[:-n] # 拆分训练集和测试集 X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0, shuffle=False) # 打印训练集和测试集的大小 print(f\u0026#34;训练集和测试集大小：{len(X_train)}, {len(X_test)}\u0026#34;) # 使用管道进行特征缩放和模型训练 pipe = Pipeline([(\u0026#34;scaler\u0026#34;, MinMaxScaler()), (\u0026#34;regressor\u0026#34;, SVR(kernel=\u0026#39;rbf\u0026#39;, C=1e3, gamma=0.1))]) pipe.fit(X_train, y_train) # 预测测试集 y_pred = pipe.predict(X_test) # 输出模型得分 print(f\u0026#39;训练集准确率：{pipe.score(X_train, y_train):0.4f}\u0026#39;) print(f\u0026#39;测试集准确率：{pipe.score(X_test, y_test):0.4f}\u0026#39;) # 使用时间序列交叉验证 tscv = TimeSeriesSplit(n_splits=5) # 获取参数列表 pipe.get_params() # 网格搜索和拟合模型 param_grid = { \u0026#34;regressor__C\u0026#34;: [0.1, 1, 10, 100, 1000], \u0026#34;regressor__kernel\u0026#34;: [\u0026#34;poly\u0026#34;, \u0026#34;rbf\u0026#34;, \u0026#34;sigmoid\u0026#34;], \u0026#34;regressor__gamma\u0026#34;: [1e-7, 1e-4, 1e-3, 1e-2] } gs = GridSearchCV(pipe, param_grid, n_jobs=-1, cv=tscv, verbose=1) gs.fit(X_train, y_train) # 输出最佳模型得分 print(f\u0026#39;训练集准确率：{gs.score(X_train, y_train):0.6f}\u0026#39;) print(f\u0026#39;测试集准确率：{gs.score(X_test, y_test):0.6f}\u0026#39;) # 创建 DataFrame 来包含关键值 df3 = pd.DataFrame({\u0026#39;X\u0026#39;: X_test.flatten(), \u0026#39;y\u0026#39;: y_pred}) df3[\u0026#39;X\u0026#39;] = df3[\u0026#39;X\u0026#39;].shift(-n) df3[\u0026#39;X-y\u0026#39;] = df3[\u0026#39;X\u0026#39;] - df3[\u0026#39;y\u0026#39;] df3 = df3[:-n] # 检查缺失值 df3.isnull().sum() # 输出均值差异 print(f\u0026#39;均值差异：{np.mean(df3[\u0026#34;X-y\u0026#34;]):0.4f}\u0026#39;) # 可视化预测结果和残差 fig, ax = plt.subplots(2, 2, figsize=(20, 10)) ax[0, 0].scatter(df3[\u0026#39;X\u0026#39;], df3[\u0026#39;y\u0026#39;]) ax[0, 0].set_title(\u0026#39;5天后的价格 vs 预测价格\u0026#39;, fontproperties=myFont) ax[0, 1].plot(df3.index, y_pred[:-n], \u0026#39;crimson\u0026#39;) ax[0, 1].set_title(\u0026#39;预测的价格\u0026#39;, fontproperties=myFont) ax[1, 0].plot(df3.index, df3[\u0026#39;X-y\u0026#39;]) ax[1, 0].set_title(\u0026#39;5天后的价格与预测价格的差异\u0026#39;, fontproperties=myFont) ax[1, 1].hist(df3[\u0026#39;X-y\u0026#39;], bins=50, density=False, color=\u0026#39;orange\u0026#39;) ax[1, 1].set_title(\u0026#39;5天后的价格与预测价格差异的直方图\u0026#39;, fontproperties=myFont) plt.show() 总结 支持向量机（SVM）的目标函数取决于处理的数据类型（线性可分、线性不可分、非线性）和相应的优化策略。以下是这三种情况下 SVM 的目标函数：\n线性可分支持向量机（Hard Margin SVM） 对于线性可分的数据集，目标是找到一个最优的超平面，即最大化两个类别之间的间隔。\n目标函数:$\\min_{\\mathbf{w}, b} \\frac{1}{2} ||\\mathbf{w}||^2$\n约束条件: $y_i (\\mathbf{w} \\cdot \\mathbf{x}_i + b) \\geq 1, \\quad \\forall i$\n线性不可分支持向量机（Soft Margin SVM） 当数据线性不可分时，通过引入“松弛变量”（slack variables）实现间隔和分类违规之间的平衡。\n目标函数: $\\min_{\\mathbf{w}, b, \\xi} \\frac{1}{2} ||\\mathbf{w}||^2 + C \\sum_{i=1}^n \\xi_i$\n约束条件: $y_i (\\mathbf{w} \\cdot \\mathbf{x}_i + b) \\geq 1 - \\xi_i, \\quad \\xi_i \\geq 0, \\quad \\forall i$\n非线性支持向量机 对于非线性数据集，目标函数和约束条件与线性不可分 SVM 类似，但使用核技巧在高维空间中寻找最优超平面。\n目标函数: $\\max_{\\alpha} \\sum_{i=1}^n \\alpha_i - \\frac{1}{2} \\sum_{i,j=1}^n y_i y_j \\alpha_i \\alpha_j K(\\mathbf{x}_i, \\mathbf{x}_j)$\n约束条件: $\\sum_{i=1}^n y_i \\alpha_i = 0 \\text{ and } 0 \\leq \\alpha_i \\leq C, \\quad \\forall i$ \u0026quot;\u0026quot;\u0026quot;\n非线性 SVM 的优势与局限性 总结非线性 SVM 在处理复杂数据集时的优势，以及其面临的挑战和局限性。\n往期推荐 一文看懂机器学习 机器学习-房价预测建模 机器学习 | 基础术语与符号 机器学习 | 特征缩放 机器学习| K 近临(K Nearest-Neighbours) 机器学习| K邻近疾病预测演示 机器学习 | K均值聚类(K-means Clustering) 机器学习 | 朴素贝叶斯原理实战 机器学习 | 线性回归 机器学习 | 支持向量机线性可分 机器学习 | 支持向量机线性不可分 欢迎扫码关注公众号，订阅更多文章!\n","permalink":"https://www.devean.cn/post/2023-12-02-machine-learning-nonlinear-support-vector-machines/","title":"机器学习 | 非线性支持向量机"},{"contents":" 本文从支持向量机线性不可分、软间隔、松弛变量、目标函数、约束条件、超参数 C,实际应用场景判别线性是否可分等几方面讲概述了支持向量机线性不可分\n线性不可分 SVM 有些时候数据本身存在噪点或异常值、在这种场景下,支持向量机又会如何处理呢,看下图\n在这里，我们在红球的边界中有一个蓝球。那么 SVM 是如何对数据进行分类的呢？这很简单！红色球边界中的蓝色球是蓝色球的异常值。SVM 算法具有忽略异常值并找到使边际最大化的最佳超平面的特点。SVM 对异常值具有鲁棒性。\n在这种类型的数据点中，SVM 所做的就是像之前的数据集一样找到最大边距，并在每次点跨越边距时添加惩罚。因此，此类情况下的边距称为软边距。当数据集存在软边距时，SVM 会尝试最小化(1/margin+∧(Σpenalty))。\n引入松弛变量 首先，引入松弛变量（Slack Variables），这些变量表示数据点到超平面的距离。对于每个数据点，引入一个对应的松弛变量 $\\xi_i$，表示第 $i$ 个数据点允许的错误。\n目标函数修改 软间隔的目标函数通过调整传统硬间隔的目标函数，以考虑错误和松弛变量。新的目标函数可以表示为：\n$ \\min_{w, b, \\xi} \\frac{1}{2}||w||^2 + C \\sum_{i=1}^{N} \\xi_i $\n其中：\n$||w||^2$ 表示模型复杂度，即超平面的法向量的范数。 $\\sum_{i=1}^{N} \\xi_i$ 表示所有松弛变量的总和。 $C$ 是一个用户定义的超参数，用于平衡最小化模型复杂度和最小化分类错误的目标。\n约束条件调整 随着引入了松弛变量，约束条件也需要相应的调整。约束条件现在变为：\n$ y_i(w \\cdot x_i + b) \\geq 1 - \\xi_i, \\quad \\text{for } i = 1, 2, \\ldots, N $\n这确保了即使数据点落在错误的一侧，它们的函数间隔仍然至少为 $1 - \\xi_i$。\n超参数调整 软间隔的效果受到超参数 $C$ 的影响， $C$ 的选择取决于对模型性能的需求。较大的 $C$ 值会更强调正确分类，但可能导致过拟合，而较小的 $C$ 值会更注重找到更大间隔，但可能容忍更多的错误分类。通过在实际问题中调整 $C$ 的值，可以根据具体情况平衡模型的复杂性和对错误的容忍度。这种灵活性使得软间隔成为处理线性不可分数据的有力工具。\n实际应用 在实际数据集上应用软间隔 SVM 时，通常需要通过试验来确定最佳的超参数 $C$。这可能需要使用网格搜索或随机搜索等技术。 软间隔 SVM 特别适用于那些数据本身就包含噪声或异常值的情况。它通过牺牲一些训练准确性，增加了模型对未见数据的泛化能力。\n判断是否完全线性可分 可视化数据：如果数据维度较低（如二维或三维），可以通过可视化来初步判断数据是否可能线性可分。\n尝试线性模型：在不考虑松弛变量的情况下，使用线性 SVM 或其他线性模型对数据进行拟合。如果模型表现良好，这可能表明数据是线性可分的。\n评估模型性能：使用交叉验证等方法来评估线性模型的性能。如果线性模型的性能不佳，可能意味着数据不是完全线性可分的。\n分析误差类型：查看模型的误差类型，如是否存在系统性误差，这可能表明数据结构的非线性特性。\n决定是否引入参数 C 处理不完全线性可分的数据：如果数据不是完全线性可分的，引入 C 是必要的。这有助于控制模型对于误分类的惩罚强度。\n防止过拟合：参数 C 可以帮助控制模型的复杂度，从而防止过拟合。特别是在数据量不是很大的情况下，合适的 C 值尤为重要。\n模型调优：通过网格搜索、随机搜索或基于模型的搜索方法（如贝叶斯优化）来找到最优的 C 值。这通常是通过交叉验证完成的。\n实验和验证：不同的 C 值可能会导致模型性能显著不同。实验不同的 C 值，并通过验证集或测试集来评估模型性能。\n平衡偏差和方差：选择 C 值是平衡模型偏差和方差的一个重要步骤。较小的 C 值可能导致高偏差（欠拟合），而较大的 C 值可能导致高方差（过拟合）。\n实战 数据加载预处理 # 导入所需的库 import numpy as np import matplotlib.pyplot as plt from sklearn.datasets import make_moons from sklearn.preprocessing import StandardScaler from sklearn.svm import SVC from sklearn.model_selection import train_test_split, GridSearchCV from sklearn.metrics import classification_report, confusion_matrix import seaborn as sns # 1. 数据加载 X, y = make_moons(n_samples=300, noise=0.2, random_state=42) # 2. 特征工程 - 标准化处理 scaler = StandardScaler() X_scaled = scaler.fit_transform(X) # 3. 探索性数据分析（EDA） plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.RdYlBu) plt.title(\u0026#34;EDA - Moon Dataset\u0026#34;) plt.xlabel(\u0026#34;Feature 1\u0026#34;) plt.ylabel(\u0026#34;Feature 2\u0026#34;) plt.show() # 4. 模型训练和评估 # 划分训练集和测试集 X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42) # 创建 SVM 模型 model = SVC(kernel=\u0026#39;rbf\u0026#39;, C=1, gamma=1) # 训练模型 model.fit(X_train, y_train) # 预测和评估 y_pred = model.predict(X_test) print(classification_report(y_test, y_pred)) # 5. 参数调优 - 使用网格搜索 param_grid = {\u0026#39;C\u0026#39;: [0.1, 1, 10, 100], \u0026#39;gamma\u0026#39;: [1, 0.1, 0.01, 0.001]} grid = GridSearchCV(SVC(), param_grid, refit=True, verbose=2) grid.fit(X_train, y_train) # 输出最佳参数 print(\u0026#34;Best Parameters:\u0026#34;, grid.best_params_) # 6. 可视化模型性能 # 绘制混淆矩阵 cm = confusion_matrix(y_test, grid.predict(X_test)) sns.heatmap(cm, annot=True, fmt=\u0026#39;g\u0026#39;) plt.title(\u0026#34;Confusion Matrix\u0026#34;) plt.xlabel(\u0026#34;Predicted label\u0026#34;) plt.ylabel(\u0026#34;True label\u0026#34;) plt.show() 往期推荐 一文看懂机器学习 机器学习-房价预测建模 机器学习 | 基础术语与符号 机器学习 | 特征缩放 机器学习| K 近临(K Nearest-Neighbours) 机器学习| K邻近疾病预测演示 机器学习 | K均值聚类(K-means Clustering) 机器学习 | 朴素贝叶斯原理实战 机器学习 | 线性回归 机器学习 | 支持向量机线性可分 欢迎扫码关注公众号，订阅更多文章!\n","permalink":"https://www.devean.cn/post/2023-11-19-machine-learning-support-vector-machine-non-linearly-separable/","title":"机器学习 | 支持向量机线性不可分"},{"contents":" 本文从支持向量机概念、硬间隔、软间隔和非线性的区别、原理、术语、最大间隔数学推导几个方面详细讲解线性可分的支持向量机。\n基础概念 支持向量机(Support Vector Machine)SVM，是一种监督学习模型，适用于二分类任务。SVM 算法的主要目标是在 N 维空间中找到能够将特征空间中不同类的数据点分开的最优超平面。超平面尝试使不同类的最近点之间的间隔尽可能最大。超平面的维度取决于特征的数量。如果输入特征的数量是两个，那么超平面只是一条线。如果输入特征的数量为三个，则超平面变为二维平面。当特征数量超过三个时就变得难以想象。\n硬间隔、软间隔和非线性SVM区别 硬间隔数据是完全准确可分的、不存在分类错误的情况，软间隔是允许一定量的样本分类错误、而线性不可分是线性公式无解，只能使用非线性方式求解的\n线性可分 SVM 即假设样本数据是线性可分离的情况下，我们直接使用线性 SVM 对数据进行分类。基本原理是找到一个超平面，将数据划分为两个类，使得类别之间的间隔最大化。\n定义 给定训练样本集 D={(x1,y1)，(x2,y2)，\u0026hellip;，(xm,ym)},yi∈{-1,+1}，分类学习最基本的想法就是基于训练集 D 在样本空间中找到一个划分超平面，将不同类别的样本分开。下面列子以二维数据展开、实际生活中多维数据与二维几乎无差别，只是数据特征维度变为了多维。\n直观上看，应该去找位于两类训练样本“正中间”的划分超平面，即图中红色的那个，因为该划分超平面对训练样本局部扰动的“容忍”性最好。如果，由于训练集的局限性或噪声的因素，训练集外的样本可能比图中的训练样本更接近两个类的分隔界，这将使许多划分超平面出现错误，而红色的超平面受影响最小。\n要找到最佳超平面,即得找到样本数据点离超平面最近的点的距离最大化，从上面两个图中可以看出,离图中红线最近的点即被圈住的样本到红色超平面距离最大。其中 w=(w1;w2;\u0026hellip;;wd)为法向量，决定了超平面的方向；b 为位移项，决定了超平面与原点之间的距离。\n术语 向量距离：即上图中任何一个样本点到红色超平面的距离 $$r= \\frac {w^Tx+b} {||w||}$$\n支持向量（support vector）：即图中圈中的样本,支持向量是距离超平面最近的数据点，在决定超平面和边距方面起着关键作用。\n决策边界:即上图中红色超平面,用于分隔特征空间中不同类的数据点。在线性分类的情况下，它将是一个线性方程 $$w^Tx+b=0$$\n上边界：将超平面放大 n 背后\n$$w^Tx+b=1$$\n下边界： $$w^Tx+b=-1$$\n间隔(margin)：即如图上下边界之间的距离\n$$\\lambda=\\frac {2} {||w||}$$\n求解线性 SVM 决策超平面 1. 列出知超平面方程组 $$w^Tx+b=0$$ $$w^Tx+b=1$$ $$w^Tx+b=-1$$\n2 假设正负超平面向量 假设正决策超平面上的存在点 $x_m$、负决策超平面上存在点 $x_n$, 求两点之间的向量可如下图\n3.可将正负超平面上的向量带入方程计算\n$$\\vec w_m \\vec x+b=1$$ $$\\vec w_n \\vec x+b=-1$$\n$$\\vec {w} \\cdot (\\vec x_m- \\vec x_n)=2$$\n4.在决策超平面上假设存在两点 $x_p,x_q$\n$$\\vec w_p \\vec x+b=0$$ $$\\vec w_q \\vec x+b=0$$\n$$\\vec {w} \\cdot (\\vec x_p- \\vec x_q)=0$$\n5. 由此 我们可推出向量 $\\vec w$ 与超平面垂直，即为超平面的法向量\n6.基于向量定理计算 根据已知,及上图结论，我们可推导出 $$\\vec {w} \\cdot (\\vec x_m- \\vec x_n)=2$$ $$||\\vec x_m- \\vec x_n|| * cos \\theta * ||\\vec {w} || =2$$\n7. 推导间隔 L 从上图中可以看出向量 $\\vec x_m- \\vec x_n$ 投影到法向量 $\\vec w$ 上，就等于间隔 L\n$$||\\vec x_m- \\vec x_n|| * cos \\theta =L$$\n$$L * ||\\vec {w} || =2$$\n$$L = \\frac {2} {||\\vec {w} ||}$$\n8.定义问题 我们的目标是最大化分类间隔，即最大化 $\\frac{2}{|w|}$。等价地，我们最小化 $\\frac{1}{2} |w|^2$。优化问题可以写成：\n最小化:\n$$ \\frac{1}{2} |w|^2 $$\n约束条件:\n$$ y_i(w \\cdot x_i + b) \\geq 1 \\quad \\text{for all } i = 1, 2, \\ldots, m $$\n9. 引入拉格朗日乘子：\n引入拉格朗日乘子 $\\alpha_i \\geq 0$，定义拉格朗日函数：\n拉格朗日方程:\n$$ L(w, b, \\alpha) = \\frac{1}{2} |w|^2 - \\sum_{i=1}^{m} \\alpha_i [y_i(w \\cdot x_i + b) - 1] $$\n10. 求解对偶问题：\n通过对拉格朗日函数对 $w$ 和 $b$ 求偏导数，并令其等于零，我们得到：\n偏导数与置换:\n$$ w = \\sum_{i=1}^{m} \\alpha_i y_i x_i $$\n对偶问题:\n$$ \\text{maximize} \\quad W(\\alpha) = \\sum_{i=1}^{m} \\alpha_i - \\frac{1}{2} \\sum_{i,j=1}^{m} \\alpha_i \\alpha_j y_i y_j x_i \\cdot x_j $$\n$$ \\text{subject to} \\quad \\alpha_i \\geq 0, \\quad \\sum_{i=1}^{m} \\alpha_i y_i = 0 $$\n11. 最大化间隔的数学表达：\n通过对偶问题的求解，得到一组优化的拉格朗日乘子 $\\alpha^*$。最大化分类间隔的数学表达式为：\n最大间隔:\n$$ \\text{maximize} \\quad \\frac{2}{|w|} = \\frac{2}{\\sqrt{\\sum_{i=1}^{m} (\\alpha_i^* y_i x_i)^2}} $$\n12. 计算最大间隔：\n最大间隔的计算是通过对偶问题中的 $\\alpha^$ 计算得到的。具体地，最大间隔是 $\\frac{2}{|w|}$，其中 $w$ 由 $\\sum_{i=1}^{m} \\alpha_i^ y_i x_i$ 计算得到。\n欢迎大家关注 往期推荐 一文看懂机器学习\n机器学习-房价预测建模\n机器学习 | 基础术语与符号\n机器学习 | 特征缩放\n机器学习| K 近临(K Nearest-Neighbours)\n机器学习| K邻近疾病预测演示\n机器学习 | K均值聚类(K-means Clustering)\n机器学习 | 朴素贝叶斯原理实战\n机器学习 | 线性回归\n","permalink":"https://www.devean.cn/post/2023-11-18-machine-learning-support-vector-machine-linearly-separable/","title":"机器学习 | 支持向量机线性可分"},{"contents":" 本文简单科普了什么是套利、有哪些套利方法、套利失效的原因。\n什么是套利 套利是指在无风险回报率之外获得确定的利润。 用量化金融的语言来说，我们 可以说套利机会是今天价值为零但未来价值为正的投资组合 的概率，并且未来为负值的概率为零。\n市场上不存在套利机会的假设是经典金融理论的基础。这个想法是俗话说“天下没有免费的午餐”。\n现在我们可以看到，我们可以想到几种套利方式。以下是最重要的几种套利的列表和说明。\n静态套利是一种无需重新平衡正向持仓的套利。 动态套利是一种需要在未来进行交易的价差套利策略，通常取决于市场状况。 统计套利并非套利，而只是根据过去的统计数据预测，可能获得超过无风险收益的利润（甚至可能根据承担的风险进行适当调整）。 模型无关套利是一种不依赖于任何金融工具数学模型而发挥作用的套利。例如，可利用的对看跌期权平价的违反或对债券与掉期之间关系的违反 依赖于模型的套利需要一个模型。例如，由于不正确的波动率估计，期权定价错误。为了从套利中获利，你需要delta对冲，这需要一个模型。 这有几个套利失效的原因\n报价错误或不可交易 期权价格和股票价格没有同步报价 存在您未计算在内的竞价价差 你的模型是错误的，或者存在你没有考虑到的风险因素 What Is Arbitrage Arbitrage is making a sure profit in excess of the risk-free rate of return. In the language of quantitative finance we can say that an arbitrage opportunity is a portfolio of zero value today which is of positive value in the future with positive probability,and of negative value in the future with zero probability.\nThe assumption that there are no arbitrage opportunities in the market is fundamental to classical finance theory.This idea is popularly known as \u0026rsquo;there\u0026rsquo;s no such thing as a free lunch.\nNow we can see that there are several types of arbitrage that we can think of. Here is a list and description of the most important.\nA static arbitrage is an arbitrage that does not require re-balancing of positives A dynamic arbitrage is an arbitrage that requires trading instruments in the future,generally contingent on market states A statistical arbitrage is not an arbitrage that but simply a likely profit in excess of the risk-free return (perhaps even suitably adjusted for risk taken) as predicted by past statistics Model-independent arbitrage is an arbitrage which dose not depend on any mathematical model of financial instruments to work. For example, an exploitable violation of put-call parity or a violation of the relationship between bonds and swaps Model-dependent arbitrage dose require a model. For example, options mis-priced because of incorrect volatility estimate. To profit from the arbitrage you need to delta hedge, and that requires a model. Here are several reasons for arbitrage fails\nQuoted prices are wrong or not tradeable Option and stock prices were not quoted synchronously There is a bid-off spread you have not accounted for Your model is wrong, or there is a risk factor you have not accounted for [1] Wilmott，M 2009 Frequently Asked Questions In Quantitative Finance, second edition. John Wiley \u0026amp; Sons,Ltd.\n欢迎扫码关注公众号，订阅更多文章!\n","permalink":"https://www.devean.cn/post/2023-11-15-what-is-arbitrage/","title":"什么是套利 | 翻译"},{"contents":" 本文量从量化金融中数学的应用、常见的建模方法、常见的数学工具普及了量化金融中的应用。\n量化金融中有哪些不同类型的数学？ 在数量金融中使用最多的数学领域是概率论和微分方程。当然，通常需要数值方法来生成数字。\n这里列出了各种建模方法和一些有用的工具。“建模方法”和“工具”之间的区别将开始变得清晰。\n建模方法\n概率模型 决策模型 离散：差分方程 连续：微分方程 实用工具\n模拟 离散化方法 近似值 渐进分析 级数解 格林函数 虽然这些清单并非完全是任意列出的，但可以接受一些批评或补充。让我们先来看看建模方法。\nWhat are the different types of Mathematics found in quantitative finance? The field of mathematics most used in quantitative finance are those of probability theory and differential equations. And,of course, numerical methods are usually needed for producing numbers.\nHere‘s a list of the various approaches to modelling and a selection of useful tools. The distinction between a \u0026lsquo;modelling approach\u0026rsquo; and a \u0026rsquo;tool\u0026rsquo; will start to become clear.\nModelling approaches: Probabilistic Deterministic Discrete: difference equations Continuous: differential equations Useful tools Simulations Discretization methods Approximations Asymptotic analysis Series solutions Green\u0026rsquo;s functions While these are not exactly arbitrary lists, they are certainly open to some criticism or addition. Let\u0026rsquo;s first take a look at the modelling approaches.\n[1] Wilmott，M 2009 Frequently Asked Questions In Quantitative Finance, second edition. John Wiley \u0026amp; Sons,Ltd.\n","permalink":"https://www.devean.cn/post/2023-11-14-mathematical-applications-in-quantitative-finance/","title":"量化金融中的数学应用 | 翻译"},{"contents":" 本文从特征缩放概念、目的、常用特征缩放方法：最小-最大缩放、标准缩放、鲁棒缩放、L2 Normalization、L1 Normalization、Power Transformer的公式讲解、Python缩放数据可视化对比诠释了特征缩放\n什么是特征缩放 特征缩放又称归一化，是机器学习中的一种技术，涉及调整数值数据的量度，使所有数据点在相似的尺度上。例如：身高、体重、年龄、收入等个人特征数据，每个维度的区间不一样，为保证所有维度的特征数据尺度一样，我们就需要对原始数据做特征缩放，将身高、体重、年龄、收入都转化为区间[0,1]之间的数据。\n为什么要做特征缩放 收敛速度：梯度下降等迭代方法在各特征尺度一致时会更快地收敛。 避免数值不稳定性：在某些算法中，如果特征的尺度差异很大，可能会导致数值计算问题。 更好的模型解释性：当所有特征都在同一个尺度上，它们的权重可以更容易地相互比较。 目的 使数据均匀：数据缩放通过将数据转换到新的尺度上，使不同特征间的数值大小差异减小。 提高算法性能：缩放可以加快梯度下降的收敛速度，并提高算法（如支持向量机和 K 近邻算法）的性能。 特征缩放方法 最小-最大缩放 (Min-Max Scaling)\n公式: $X_{norm} =\\frac {X - X_{min}} {X_{max} - X_{min}}$ 描述: 将数据缩放到[0,1]范围内的技术。 场景: 当数据分布不是高度偏斜，并且不包含极端值时。 标准化 (Standardization)\n公式: $X_{standard} =\\frac {(X - μ)} σ$ 描述: 通过使数据的平均值为 0，标准差为 1 来缩放数据。 场景：当算法需要数据的标准差为 1，且偏差很小时。 稳健缩放 (Robust Scaling)\n公式: $X_{robust} =\\frac {X - Q1} {Q3 - Q1}$ 描述: 缩放技术，可以减少离群值的影响。 场景：当数据包含许多离群值或异常值时。 L2 Normalization (欧几里得范数)\n公式: $X_{l2} =\\frac {X - μ} {||X||_2}$ 描述: 通过使特征向量的欧几里得长度为 1 来缩放特征。 场景：在图像处理和文本分类中，当数据的方向比其大小更重要时。 L1 Normalization\n公式: $X_{l1} =\\frac {X - μ} {||X||_1}$ 描述: 通过使特征向量的欧几里得长度为 1 来缩放特征。 场景：在图像处理和文本分类中，当数据的方向比其大小更重要时。 Power Transformer a. Box-Cox Transformation\n公式: $X_{Box-Cox} =\\frac {X^{\\lambda}-1} {\\lambda},for \\quad \\lambda \\neq 0,ln(X) \\quad for \\quad \\lambda=0$ 描述: Box-Cox 转换只能应用于正值的数据。它的目标是对非常数方差和非正态分布的数据进行变换，使其更接近正态分布。 场景： 线性模型：在回归、ANOVA 或设计实验中，当我们希望满足线性模型的正态分布假设时。 时间序列分析：稳定化时间序列数据的方差。 方差稳定化：在很多统计模型中，稳定的方差是关键。Box-Cox 转换能够稳定化方差，使其不随因变量的值而变化。 处理倾斜数据：对于正偏斜或负偏斜的数据，Box-Cox 转换可以帮助减少偏斜。 b. Yeo-Johnson Transformation\n公式： $X_{yeo-Johnson}=\\frac {(X+1)^{\\lambda}-1} {\\lambda} \\quad for \\quad \\lambda \\neq 0 \\quad and \\quad X \\geq 0,ln(X+1) \\quad for \\quad\\lambda=0 \\quad and \\quad X \\geq 0，-\\frac {(-X+1)^{\\lambda}-1} {\\lambda} \\quad for \\quad \\lambda \\neq 0 \\quad and \\quad X \u0026lt; 0, -ln(-X+1) \\quad for \\quad \\lambda=0 \\quad and \\quad X \u0026lt;0$ 描述: Yeo-Johnson 转换是 Box-Cox 的扩展，它可以应用于正值、负值和零的数据。这种转换同样旨在使数据更接近正态分布。 场景： 广义线性模型：当我们需要满足广义线性模型的正态分布假设时。 包含零或负值的数据：与 Box-Cox 不同，Yeo-Johnson 转换可以应用于包含零或负值的数据。 方差稳定化：与 Box-Cox 类似，Yeo-Johnson 转换也可以用来稳定化方差。 处理倾斜数据：对于正偏斜或负偏斜的数据，Yeo-Johnson 转换也是一个有效的工具。 数据缩放对比 原始数据 ID 身高(cm) 体重(kg) 心率(bpm) 胆固醇(mg/dL) 年龄 脚长(cm) 1 170 68 75 180 25 25 2 160 50 80 200 30 23 3 180 77 72 220 28 27 4 175 65 78 210 32 26 5 166 58 82 190 29 24 python 代码 import pandas as pd import numpy as np import matplotlib.pyplot as plt from sklearn.preprocessing import (MinMaxScaler, StandardScaler, RobustScaler, MaxAbsScaler, QuantileTransformer, PowerTransformer) from matplotlib.font_manager import FontProperties # 设置中文字体路径 myFont = FontProperties(fname=\u0026#39;/System/Library/Fonts/PingFang.ttc\u0026#39;) # Create the dataset data = { \u0026#39;身高 (cm)\u0026#39;: [170, 175, 168, 180, 172], \u0026#39;体重 (kg)\u0026#39;: [65, 72, 58, 80, 68], \u0026#39;年龄\u0026#39;: [25, 30, 28, 35, 29], \u0026#39;脚长 (cm)\u0026#39;: [25, 26, 25, 27, 26], \u0026#39;收缩压 (mmHg)\u0026#39;: [120, 125, 118, 128, 121], \u0026#39;胆固醇 (mg/dL)\u0026#39;: [190, 200, 185, 210, 195], \u0026#39;心率 (bpm)\u0026#39;: [70, 72, 68, 75, 71] } df = pd.DataFrame(data) # Define a unique color for each feature colors = plt.cm.Accent(np.linspace(0, 1, len(df.columns))) # Scaling methods scalars = { \u0026#39;原始数据\u0026#39;: None, \u0026#39;最小-最大 缩放器\u0026#39;: MinMaxScaler(), \u0026#39;标准缩放器\u0026#39;: StandardScaler(), \u0026#39;鲁棒缩放器\u0026#39;: RobustScaler(), \u0026#39;最大绝对值缩放器\u0026#39;: MaxAbsScaler(), \u0026#39;分位数转换器\u0026#39;: QuantileTransformer(n_quantiles=5), \u0026#39;幂转换器\u0026#39;: PowerTransformer(method=\u0026#39;yeo-johnson\u0026#39;) } # Determine the number of rows and columns for the subplots n_features = len(df.columns) n_scalars = len(scalars) n_rows = n_features n_cols = n_scalars fig, axes = plt.subplots(nrows=n_rows, ncols=n_cols, figsize=(20, 4 * n_features)) for row, feature in enumerate(df.columns): for col, (name, scaler) in enumerate(scalars.items()): ax = axes[row, col] if scaler: scaled_data = scaler.fit_transform(df[[feature]]) else: scaled_data = df[[feature]].values ax.hist(scaled_data, bins=10, color=colors[row], edgecolor=\u0026#39;black\u0026#39;) if row == 0: ax.set_title(name, fontproperties=myFont) if col == 0: ax.set_ylabel(feature, fontproperties=myFont, rotation=0, labelpad=60, ha=\u0026#39;right\u0026#39;) fig.text(0.5, 0.01, \u0026#39;Value\u0026#39;, ha=\u0026#39;center\u0026#39;, fontproperties=myFont) fig.text(0.01, 0.5, \u0026#39;Frequency\u0026#39;, va=\u0026#39;center\u0026#39;, rotation=\u0026#39;vertical\u0026#39;, fontproperties=myFont) # Adjust layout plt.tight_layout(pad=1.0) plt.subplots_adjust(top=0.90,left=0.12, wspace=0.4, hspace=0.5,bottom=0.08) plt.suptitle(\u0026#34;各特征在不同缩放方法下的分布\u0026#34;, fontproperties=myFont) plt.show() 特征缩放图示 欢迎扫码关注公众号，订阅更多文章!\n","permalink":"https://www.devean.cn/post/2023-11-11-machine-learning-feature-scaling/","title":"机器学习 | 特征缩放"},{"contents":" 本文从概念、原理、距离函数、K 值选择、K 值影响、、优缺点、应用几方面详细讲述了 KNN 算法。\nK 近临(K Nearest-Neighbours) 一种简单的监督学习算法，惰性学习算法，在技术上并不训练模型来预测。适用于分类和回归任务。它的核心思想是：相似的对象彼此接近。例如，若果你想分类一个新的数据点(绿点)，可以查看训练数据中哪些数据点与它最接近，并根据这些最接近的数据点和标签来预测它的标签(红点或蓝圆)。\n概念 K: 这是一个用户指定的正整数，即训练数据分类数量，代表要考虑的最近邻居的数量，上图中假设 K=2,即训练数据分类为蓝色圆和红色三角两类标签。\n距离函数: 用于计算数据点之间的距离。最常见的是欧几里得距离、曼哈顿距离、马氏距离等。\n投票机制:\n分类任务: 将根据 k 个最近邻的多数投票来确定新数据点的类别。 回归任务: 通常取 k 个最近邻的输出变量的平均值。 原理 距离计算： 对于给定的新数据点，计算它与训练数据集中每个点的距离。 选取 K 个邻居： 从训练数据集中选取距离最近的 K 个点。 投票 (对于分类)： 对于 K 个邻居，看哪个类别最为常见，并将其指定为新数据点的类别。 均值 (对于回归)： 对于 K 个邻居，计算其属性的平均值，并将其指定为新数据点的值。 距离度量 欧几里得距离 (Euclidean Distance) 欧几里得距离的名称来源于古希腊数学家欧几里得，是衡量两点在平面或更高维空间中的\u0026quot;直线\u0026quot;距离。它基于勾股定理，用于计算两点之间的最短距离。在日常生活中，我们经常无意识地使用欧几里得距离，例如，当我们说两地之间的\u0026quot;直线\u0026quot;距离时，实际上是在引用欧几里得距离。 公式: 给定两点 P 和 Q，其坐标分别为 $P(x_1, x_2, \u0026hellip;, x_n)$ 和 $Q(y_1, y_2, \u0026hellip;, y_n)$ 在一个 n 维空间中，它们之间的欧几里得距离 d 定义为：\n$d(P, Q) = \\sqrt{\\sum_{i=1}^{n} (x_i - y_i)^2}$\n曼哈顿距离 (Manhattan Distance) 曼哈顿距离得名于纽约的曼哈顿，因为在曼哈顿的街道布局是网格状的。想象一下，你在一个街区的一个角落，要走到对面的角落，你不能直接穿越街区，只能沿着街道走。这就是曼哈顿距离的来源，也因此它有时被称为“城市街区距离”。\n公式\n给定两点 P 和 Q，其坐标分别为 $P(x_1, x_2, \u0026hellip;, x_n)$ 和 $Q(y_1, y_2, \u0026hellip;, y_n)$ 在一个 n 维空间中，它们之间的曼哈顿距离 L1 定义为：\n$L1(P, Q) = \\sum_{i=1}^{n} |x_i - y_i|$\n闵可夫斯基距离 (Minkowski Distance) 闵可夫斯基距离是一种在向量空间中度量两个点之间距离的方法。它实际上是一种泛化的距离度量，可以看作是其他距离度量（如欧几里得距离、曼哈顿距离）的泛化。通过改变一个参数p，它可以表示多种距离度量。\n公式\n给定两点 P 和 Q，其坐标分别为 $P(x_1, x_2, \u0026hellip;, x_n)$ 和 $Q(y_1, y_2, \u0026hellip;, y_n)$ 在一个 n 维空间中，它们之间的闵可夫斯基距离 Lp 定义为：\n$Lp(P, Q) = \\left( \\sum_{i=1}^{n} |x_i - y_i|^p \\right)^{\\frac{1}{p}}$\n其中 p 是一个大于等于 1 的实数。特定的 p 值会导致其他常见的距离度量：\n当 p = 1 时，这变成了曼哈顿距离。 当 p = 2 时，这变成了欧几里得距离。 余弦相似性 (Cosine Similarity) 余弦相似性度量了两个向量方向的相似度，而不是它们的大小。换句话说，它是通过比较两个向量之间的夹角来测量它们的相似性的。夹角越小，相似性就越高。\n它经常在高维空间中（如 TF-IDF 权重的文档向量）使用，因为在高维空间中，基于欧几里得距离的相似性度量可能不太有效。\n公式 给定两个向量 A 和 B，它们的余弦相似性定义为：\n$\\text{cosine similarity}(A, B) = \\frac{A \\cdot B}{|A| |B|}$\n其中：\n$A \\cdot B$ 是向量 A 和 B 的点积。 $|A|$ 和 $|B|$ 分别是向量 A 和 B 的欧几里得长度（或模）。 公式可以进一步扩展为：\n$\\text{cosine similarity}(A, B) = \\frac{\\sum_{i=1}^{n} A_i B_i}{\\sqrt{\\sum_{i=1}^{n} A_i^2} \\sqrt{\\sum_{i=1}^{n} B_i^2}}$\n这里，n 是向量的维度，而 $A_i$ 和 $B_i$ 分别是向量 A 和 B 在第 i 维度上的值。\n余弦相似性值范围为[-1, 1]，其中 1 表示完全相似，0 表示不相关，而-1 表示完全相异。\nK 值的确定方法： 交叉验证： 这是确定 k 值的最常用方法。对于每一个可能的 k 值，使用交叉验证计算模型的预测错误率，选择错误率最低的 k 值。\n启发式方法： 有时，可以选择 sqrt(n)作为起始点，其中 n 是训练样本的数量。这只是一个粗略的估计，通常需要进一步验证。\n误差曲线： 画出不同 k 值对应的误差率曲线，选择误差变化开始平稳的点。\n领域知识： 在某些应用中，基于领域知识和经验选择 k 值可能更为合适。\nK 值的影响： 过小的 k 值： 分类：模型可能变得过于敏感和复杂。它可能对训练数据中的噪声或异常点特别敏感，从而容易过拟合。当 k=1 时，任何训练数据中的异常点都可能影响预测结果。 回归：模型可能会受到异常值的强烈影响，导致预测结果出现明显的波动。 过大的 k 值： 分类：模型可能变得过于简化。随着 k 值的增加，分类决策的边界会变得更加平滑，可能会忽视数据中的细微模式，导致欠拟合。 回归：模型同样可能会过于简化。大 k 值使模型的预测偏向于所有数据点的平均值，因此可能会忽视数据中的局部特性或细节。 优缺点 优点 简单且直观。 无需训练阶段，适用于动态变化的数据集。 对异常值不敏感（取决于 K 的大小）。\n缺点 计算复杂度高，因为对于每一个新的数据点，都需要与所有训练数据计算距离。 需要决定 K 的大小，这可能会影响结果。 高维数据中的性能下降。\n应用场景： 推荐系统:\n基于用户之前的喜好推荐相似电影 推荐用户可能喜欢的曲目或歌手 文本分类: 区分垃圾邮件和正常邮件。\n图像识别: 识别包括上的手写邮政编码，分类投递邮件包裹\n医疗诊断： 预测患者可能的疾病风险。\n信用评分：预测客户的信用风险。\n欺诈检测：识别信用卡中的异常交易。\n位置基服务：基于位置提供餐厅或服务推荐。\n","permalink":"https://www.devean.cn/post/2023-11-12-machine-learning-k-nearest-neighbours/","title":"K临近(KNN) | 机器学习"},{"contents":" 本文从机器学习与传统建模区别、机器学习分类：监督、非监督、半监督、强化，基础算法：K临近(KNN)、K均值(KMC)、朴素贝叶斯(NBC)、支持向量机(SVM)、回归、自组织映射、神经网络原理(NLP)\n什么是机器学习 机器学习是人工智能的一个分支,它让计算机从数据中自动“学”到知识,并用这些知识做决策或预测,而不需要我们一步步明确地告诉它怎么做。\n传统数学 vs 机器学习 数学建模 机器学习 相同点 数据驱动: 两者都利用数据来构建和验证模型。\n预测和推断: 数学建模和机器学习都可以用于预测未知的输出或解释数据中的模式。\n优化问题: 在某些情况下，两者都可能涉及到优化问题，例如，寻找最小化误差的参数。\n不同点 目的 数学建模：旨在用数学的形式来描述现实世界中的现象或问题，往往为了理解其背后的机制或原理。\n主要关注的是预测和泛化。机器学习模型可能不太关心背后的机制，而是关心在未知数据上的性能。\n模型构建 数学建模：模型的形式通常基于对现象的物理、生物或经济学的理解。例如，描述人口增长的模型可能基于出生率和死亡率的估计。\n机器学习：模型的形式主要基于数据。使用的模型可能没有明确的现实意义，例如深度学习模型。\n验证 数学建模：模型的验证通常基于其是否与现实世界的观察相符合，以及其是否可以提供洞察力。\n机器学习：验证通常基于模型在独立测试集上的性能。\n模型的解释性 数学建模：模型往往更具解释性，因为它们是基于现象的某些已知原理或规律构建的。\n机器学习：尤其是某些复杂的模型，如深度神经网络，可能难以解释。尽管如此，机器学习领域也有许多工作在努力提高模型的可解释性。\n应用 数学建模：常应用于工程、物理学、经济学等领域，以帮助专家了解和控制系统。\n机器学习：广泛应用于计算机视觉、自然语言处理、推荐系统等领域，主要关注自动化和预测。\n模型复杂性 数学建模：往往倾向于使用更简单的、基于物理学或其他学科原理的模型。\n机器学习：可能使用非常复杂的模型，特别是当数据量大且复杂度高时。\n总的来说，数学建模和机器学习都是理解、解释和预测现象的工具，但它们的关注点、方法和应用有所不同。\n主要类别 机器学习主要分为三大类：监督学习、非监督学习、强化学习，而监督学习和非监督学习中又衍生出半监督学习。\n监督学习 Supervised Learning是机器学习的一种方法，其模型是通过输入-输出(有标签的数据)对进行训练，目标是从给定的数据中学习一个映射函数,以便在给定新的输入时，模型可以预测相应的输出 。训练过程涉及到输入数据和其相应的标签，并尝试找到这两者之间的关系。一旦模型被训练，它可以用来预测新、未标签数据的输出。\n非监督学习 Unsupervised Learning模型被训练在没有标签的数据上。它的目的是学习数据的底层结构、分布或表示，而不是预测标签。与监督学习不同，非监督学习的目标并不是预测一个输出。相反，它试图通过某种方式学习数据的结构，这可以是通过聚类、降维或生成模型等方式来实现的。\n强化学习 Reinforcement Learning是通过与环境交互来学习如何行动，从而最大化某种定义的长期回报。与传统的监督学习不同，强化学习通常涉及决策问题，其中每个行动都会影响未来的回报。\n半监督学习 Semi-supervised Learning是介于两个极端之间(监督式是指整个数据集被标记，而非监督式是指没有标记)。半监督学习任务具有一个标记和一个未标记的数据集。它使用未标记的数据来获得对数据结构的更多理解。通常，SSL使用小的带标签数据集和较大的未带标签数据集来进行学习。)学习正如其名称所示，介于两个极端之间(监督式是指整个数据集被标记，而非监督式是指没有标记)。半监督学习任务具有一个标记和一个未标记的数据集。它使用未标记的数据来获得对数据结构的更多理解。通常，SSL使用小的带标签数据集和较大的未带标签数据集来进行学习。\n为什么会用半监督学习 半监督学习位于监督学习和非监督学习之间，利用少量的标记数据和大量的未标记数据进行学习。以下是为什么要使用半监督学习的原因：\n数据标注成本高 ：在很多应用中，收集大量数据是相对容易的，但为这些数据打标签则既昂贵又耗时。例如，在医学图像领域，一个专家可能需要花费大量时间来手动标注图像中的特定结构或病变。利用半监督学习，可以用少量的标注数据和大量的未标注数据共同训练模型。\n利用数据的完整潜力 ：未标记的数据包含有关数据分布的有用信息。半监督学习方法尝试利用这些信息来改善模型的性能。\n提高泛化能力 ：在某些情况下，利用大量的未标记数据可以帮助模型更好地泛化到新的、未见过的数据。\n数据标注可能存在误差 ：在某些场景中，即使数据被标注，标签也可能存在噪音或误差。在这种情况下，使用半监督学习方法，结合大量的未标记数据，可能会提供一个更稳健的学习策略。\n在某些任务中，有很多相关的未标记数据 ：例如，在自然语言处理中，我们可能有少量标记的数据集，但可以从网络上轻松获得大量的相关文本。半监督学习可以利用这些未标记的文本来提高模型的性能。\n适应数据的变化 ：在动态环境中，数据分布可能随时间而变化。利用半监督学习，可以定期利用新收集的未标记数据来更新模型，使其适应变化。\n总之，半监督学习提供了一种在有限标记数据的情况下利用未标记数据的方法，这对于许多实际应用来说是非常有价值的。\n模型 K近临 K Nearest-Neighbours是一种监督学习技术，给定一个新的观测值，KNN算法会从训练数据集中搜索出k个与其最相似的实例，然后基于这些邻居的属性来预测新观测值的标签。\n原理 距离计算 ：对于给定的新数据点，计算它与训练数据集中每个点的距离。\n选取K个邻居 ：从训练数据集中选取距离最近的K个点。\n投票 (对于分类) ：对于K个邻居，看哪个类别最为常见，并将其指定为新数据点的类别。\n平均 (对于回归) ：对于K个邻居，计算其属性的平均值，并将其指定为新数据点的值。\nK均值 K-Means Clustering是一种无监督的聚类算法，其目的是将n个数据点分为k个聚类。每个聚类都有一个中心，这些中心最小化了其内部数据点与中心之间的距离。\n原理 初始化： 随机选择k个数据点作为初始聚类中心。\n分配： 为每个数据点分配最近的聚类中心。\n更新： 计算每个聚类的平均值，并将平均值设为新的聚类中心。\n重复： 重复步骤2和3，直到聚类中心不再显著变化。\n朴素贝叶斯 Naive Bayes Classifier监督学习技术是基于贝叶斯定理的一种简单概率分类器。它假设特征之间是独立的（这就是“朴素”一词的来源），即一个特征的出现不会影响另一个特征的出现。\n原理 给定一个类别 C 和一个特定的特 x ，贝叶斯定理表示为：\n其中\nP(C|x) 是在给定特征 x 的情况下类别 C 的后验概率。\nP(x|C) 是在类别 C 的情况下观察到特征 x 的概率。\nP(C) 是类别 C 的先验概率。\nP(x) 是观察到特征 x 的概率。\n对于分类问题，我们可以忽略分母（因为它对所有的类别都是相同的）并计算每个类别的$P(C)×P(x∣C)$。我们将数据点分类为给出最大值的类别。\n回归方法 Regression Methods是一种预测性监督学习技术，它研究的是因变量(目标)和自变量(预测器)之间的关系。 这种技术通常用于预测分析，时间序列模型以及发现变量之间的因果关系。\n支持向量机 Support Vector Machines, SVM）是一种在分类和回归分析中使用的监督学习模型，基本思想是找到一个超平面，使得两个类别之间的边界最大化。对于线性可分的数据，这意味着超平面可以完美地分隔两个类别，并且最大化与最近的训练数据点（即支持向量）之间的距离。SVM使用所谓的核方法进行扩展。核方法的思想是将数据映射到一个更高维度的空间，使其在新的空间中变得线性可分。\n自组织映射 Self-Organizing Maps，简称SOM是一种无监督学习的神经网络，SOM是一种将高维数据映射到通常是二维（有时是三维）的网格结构上的方法。与其他神经网络不同，SOM没有激活函数，它根据输入特征的相似性将相似的输入向量组合在一起。\n原理 初始化 ：为每个节点（或称为神经元）随机初始化一个权重向量。\n竞争 ：对于每个输入样本，找到与其最相似（即欧几里得距离最近）的权重向量的节点，这个节点被称为胜者节点（winning node）。\n适应 ：更新胜者节点和其邻居节点的权重向量，使它们更接近当前输入样本。邻居节点的定义和更新的幅度都随时间逐渐减少。\n迭代 ：对于大量的迭代，重复上述步骤，直到模型收敛。\n决策树 Decision Trees是一种监督学习模型、主要用于分类和回归任务，决策树是一个树形结构，其中每个内部节点表示一个特征属性上的测试，每个分支代表一个测试结果，每个叶节点代表一个类别（在分类任务中）或连续的值（在回归任务中）。\n原理 决策树通过一系列基于特征值的测试，将输入数据点分配到叶节点中的一个，从而完成分类或预测任务。\n神经网络 Neural Networks是一种模仿生物神经网络结构和功能的计算模型。\n基本结构 神经元（Neuron） : 神经网络的基本单元。每个神经元接收一个或多个输入，加权处理这些输入，然后产生一个输出。\n层（Layer） ：神经网络由多层神经元组成。主要有三种类型的层：\n输入层（Input Layer） ：接收外部数据的层。\n隐藏层（Hidden Layers） ：在输入层和输出层之间的层，可以有多个。\n输出层（Output Layer） ：产生最终预测或分类的层。\n工作原理 每个神经元的输入都与一个权重相乘，所有加权输入的总和加上一个偏置，然后传递给激活函数。激活函数的输出是该神经元的输出。\n欢迎扫码关注公众号，订阅更多文章!\n","permalink":"https://www.devean.cn/post/2023-09-08-machine-learning-basic-concepts/","title":"机器学习 | 基础概念"},{"contents":"环新疆摩旅 版权声明：以下图片版权归本人所有，禁止一切商用，如需商用请于本人取得联系获取授权，侵权必究！ 2023.8.2 北京\u0026ndash;巴彦淖尔临河服务区 2023.8.3 巴彦淖尔临河服务区\u0026ndash;额济纳旗 2023.8.3 额济纳旗\u0026ndash;哈密 2023.8.5 哈密-巴里坤 阿勒吞古街 哈密文化馆 哈密回王墓 天山庙 巴里坤草原 2023.8.6 巴里坤-吐鲁番-乌鲁木齐 巴里坤糊 天山无人区峡谷 麻扎村 2023.8.7 乌鲁木齐 ","permalink":"https://www.devean.cn/post/2023-08-02-motorcycle-tour-around-xinjiang/","title":"环新疆摩旅"},{"contents":"Nginx 下载安装 Web服务部署 Nginx 详细配置 年费CA证书配置 安装certbot yum install certbot 手动只安装证书 certbot run -a manual -i nginx -d domain.com,www.domain.com Saving debug log to /var/log/letsencrypt/letsencrypt.log Requesting a certificate for www.domain.com - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - Create a file containing just this data: I7Sl7-xakxMT9dCS67OrHmzn_tFiBx-64g58jlkj9FM.LdWLRuDDIqZUWnece6JOlugrqigifvupPi5EXSfWi0M And make it available on your web server at this URL: http://www.yuntun.com/.well-known/acme-challenge/I7Sl7-xakxMT9dCS67OrHmzn_tFiBx-64g58jlkj9FM - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - Press Enter to Continue 创建验证文件 按上面提示创建对应的文件 自己验证http文件可访问 继续按continue 重启nginx服务 ","permalink":"https://www.devean.cn/post/2023-12-11-nginx%E9%83%A8%E7%BD%B2web%E6%9C%8D%E5%8A%A1%E5%8F%8A%E5%9F%9F%E5%90%8D%E5%85%8D%E8%B4%B9ca%E8%AE%A4%E8%AF%81%E8%AF%A6%E7%BB%86%E6%B5%81%E7%A8%8B/","title":"Nginx部署web服务及域名免费CA认证详细流程"},{"contents":"About Me Devean Ye is\n","permalink":"https://www.devean.cn/about/","title":""},{"contents":"Java 语言学习笔记 ","permalink":"https://www.devean.cn/notes/","title":""},{"contents":"","permalink":"https://www.devean.cn/archive/","title":"Posts Archive"},{"contents":"","permalink":"https://www.devean.cn/search/search/","title":"搜索"}]